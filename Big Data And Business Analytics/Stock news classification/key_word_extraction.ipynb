{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After giving each article a tag(bullish article or bearish article) [code](https://nbviewer.jupyter.org/github/popolee0513/Data-Science-Skills-Practice/blob/master/Big%20Data%20And%20Business%20Analytics/Stock%20news%20classification/Stock.ipynb),we want to find the key words in bullish article and bearish article respectively in order to build features and make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "nFTGg1sh05C9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"data_with_tag.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2O5bWmZH1Tuk",
    "outputId": "053ec1f2-d5d9-4f41-90d7-ee2f811f6613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4806, 5)\n",
      "fall    2543\n",
      "rise    2263\n",
      "Name: tag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data=data[data[\"tag\"]!=\"same\"]\n",
    "print(data.shape)\n",
    "print(pd.value_counts(data[\"tag\"]))\n",
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "j-Yd0s2U22_U",
    "outputId": "80ac1e5b-78b8-4af4-9f34-4ea6b6cfd10b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_time</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>weekday</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>新聞大立光月合併營收億元</td>\n",
       "      <td>發文前請先詳閱新聞分類發文規範未依規範發文將受處份連結過長請善用縮網址連結能不能點擊者板規處...</td>\n",
       "      <td>1</td>\n",
       "      <td>fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>新聞罕見大立光去年月營收年減</td>\n",
       "      <td>原文連結必須檢附原文內容罕見大立光去年月營收年減中央社記者韓婷婷台北日電大立光電公布去年月合...</td>\n",
       "      <td>1</td>\n",
       "      <td>fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>新聞大立光業績免驚外資喊到元</td>\n",
       "      <td>原文連結必須檢附原文內容股王大立光今天公告月營收億元月減成創下年月以來單月最低紀錄顯示蘋果光...</td>\n",
       "      <td>1</td>\n",
       "      <td>fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>公告板開始舉辦樂透</td>\n",
       "      <td>大立光請到板按參與樂透一張幣迷你級樂透結束時間</td>\n",
       "      <td>1</td>\n",
       "      <td>fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>新聞賣超差傳通路庫存爆滿</td>\n",
       "      <td>賣超差傳通路庫存爆滿蘋果砍單三成時間年月日上午聚財網新聞記者陳瑞哲報導大立光前月營收大減三成...</td>\n",
       "      <td>2</td>\n",
       "      <td>fall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    post_time           title  \\\n",
       "0  2016-01-05    新聞大立光月合併營收億元   \n",
       "1  2016-01-05  新聞罕見大立光去年月營收年減   \n",
       "2  2016-01-05  新聞大立光業績免驚外資喊到元   \n",
       "3  2016-01-05       公告板開始舉辦樂透   \n",
       "4  2016-01-06    新聞賣超差傳通路庫存爆滿   \n",
       "\n",
       "                                             content  weekday   tag  \n",
       "0  發文前請先詳閱新聞分類發文規範未依規範發文將受處份連結過長請善用縮網址連結能不能點擊者板規處...        1  fall  \n",
       "1  原文連結必須檢附原文內容罕見大立光去年月營收年減中央社記者韓婷婷台北日電大立光電公布去年月合...        1  fall  \n",
       "2  原文連結必須檢附原文內容股王大立光今天公告月營收億元月減成創下年月以來單月最低紀錄顯示蘋果光...        1  fall  \n",
       "3                            大立光請到板按參與樂透一張幣迷你級樂透結束時間        1  fall  \n",
       "4  賣超差傳通路庫存爆滿蘋果砍單三成時間年月日上午聚財網新聞記者陳瑞哲報導大立光前月營收大減三成...        2  fall  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HvaA4ICX3Wgi"
   },
   "outputs": [],
   "source": [
    "def get_ngrams(n, data):\n",
    "    tf = {}\n",
    "    df = {}\n",
    "    tfdf = {}\n",
    "    for row in range(len(data)):\n",
    "        tokens = [data.iloc[row][i:i+n] for i in range(0, len(data.iloc[row])-(n-1))]\n",
    "        #if tokens not in stopword:\n",
    "        for token in set(tokens):\n",
    "            if token not in df.keys():\n",
    "                df[token] = 1\n",
    "            else:    \n",
    "                df[token] += 1\n",
    "        for token in tokens:\n",
    "            if token not in tf.keys():\n",
    "                tf[token] = 1\n",
    "            else:\n",
    "                tf[token] += 1\n",
    "    for key, value in tf.items():\n",
    "        tfdf[key] = [value, df[key]]\n",
    "    final = pd.DataFrame.from_dict(tfdf, orient = 'index', columns = ['tf','df'])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CM67OXco5SSc"
   },
   "outputs": [],
   "source": [
    "data[\"all\"]=data[\"title\"]+data[\"content\"]\n",
    "rise=data[data[\"tag\"]==\"rise\"]\n",
    "fall=data[data[\"tag\"]==\"fall\"]\n",
    "\n",
    "data_2_gram=get_ngrams(2, data[\"all\"])\n",
    "data_3_gram=get_ngrams(3, data[\"all\"])\n",
    "data_4_gram=get_ngrams(4, data[\"all\"])\n",
    "\n",
    "fall_2_gram=get_ngrams(2, fall[\"all\"])\n",
    "fall_3_gram=get_ngrams(3, fall[\"all\"])\n",
    "fall_4_gram=get_ngrams(4, fall[\"all\"])\n",
    "\n",
    "rise_2_gram=get_ngrams(2, rise[\"all\"])\n",
    "rise_3_gram=get_ngrams(3, rise[\"all\"])\n",
    "rise_4_gram=get_ngrams(4, rise[\"all\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5UiXgRo-74ir"
   },
   "outputs": [],
   "source": [
    "data_all_gram=pd.concat([data_2_gram,data_3_gram],axis=0)\n",
    "rise_all_gram=pd.concat([rise_2_gram,rise_3_gram],axis=0)\n",
    "fall_all_gram=pd.concat([fall_2_gram,fall_3_gram],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_nsXxcGENncn",
    "outputId": "2fef4e16-4c00-4d8c-b647-3b15a29a83fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16826, 2)\n",
      "(7597, 2)\n",
      "(8793, 2)\n"
     ]
    }
   ],
   "source": [
    "data_all_gram.head()\n",
    "\n",
    "data_all_gram=data_all_gram[data_all_gram[\"tf\"]>=40]\n",
    "rise_all_gram=rise_all_gram[rise_all_gram[\"tf\"]>=40]\n",
    "fall_all_gram=fall_all_gram[fall_all_gram[\"tf\"]>=40]\n",
    "\n",
    "print(data_all_gram.shape)\n",
    "print(rise_all_gram.shape)\n",
    "print(fall_all_gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8ntMs2FYMuWn"
   },
   "outputs": [],
   "source": [
    "def remove_same(df):\n",
    "    \"\"\"移除相同DF的 被較長詞包含的詞\"\"\"\n",
    "    df['len'] = df.index.str.len()\n",
    "    df.sort_values('len', ascending=True, inplace = True)\n",
    "    df.drop('len', axis=1, inplace=True)\n",
    "    same_drop = set()\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i+1, len(df)):\n",
    "            # row i 的詞比row j 的詞短 (e.g. row i: 2-gram, row j: 3-gram)\n",
    "            # 且row i 被 row j 的詞包含\n",
    "            if (len(df.index[i]) < len(df.index[j])) & (df.index[i] in df.index[j]): \n",
    "                # 兩個詞的 DF 相差不到1% same DF number\n",
    "                if abs(df.iloc[i, 1] - df.iloc[j, 1]) <= max(df.iloc[i, 1], df.iloc[j, 1]) * 0.01 :\n",
    "                    #add the word in row i(shorter word) to a same_drop set\n",
    "                    same_drop.add(df.index[i])\n",
    "                    break\n",
    "    return df.drop(same_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Mi1Hqr1hK_4V"
   },
   "outputs": [],
   "source": [
    "data_all_gram=remove_same(data_all_gram)\n",
    "rise_all_gram=remove_same(rise_all_gram)\n",
    "fall_all_gram=remove_same(fall_all_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "f-Zw57TrLcvC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_all_gram['tfidf'] = (1+np.log(data_all_gram.tf))*np.log(4806/data_all_gram.df)\n",
    "rise_all_gram['tfidf'] = (1+np.log(rise_all_gram.tf))*np.log(2263/rise_all_gram.df)\n",
    "fall_all_gram['tfidf'] = (1+np.log(fall_all_gram.tf))*np.log(2543/fall_all_gram.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5gx8F8vcPeue"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under 1000 keyswords and in mi*tfidf mode,the accuracy=0.61026\n",
      "Under 2000 keyswords and in mi*tfidf mode,the accuracy=0.65049\n",
      "Under 3000 keyswords and in mi*tfidf mode,the accuracy=0.68793\n",
      "Under 4000 keyswords and in mi*tfidf mode,the accuracy=0.66713\n",
      "Under 1000 keyswords and in df_chi mode,the accuracy=0.72399\n",
      "Under 2000 keyswords and in df_chi mode,the accuracy=0.71429\n",
      "Under 3000 keyswords and in df_chi mode,the accuracy=0.69071\n",
      "Under 4000 keyswords and in df_chi mode,the accuracy=0.64771\n",
      "Under 1000 keyswords and in midf mode,the accuracy=0.73232\n",
      "Under 2000 keyswords and in midf mode,the accuracy=0.72399\n",
      "Under 3000 keyswords and in midf mode,the accuracy=0.69487\n",
      "Under 4000 keyswords and in midf mode,the accuracy=0.65049\n",
      "Under 1000 keyswords and in lift mode,the accuracy=0.73370\n",
      "Under 2000 keyswords and in lift mode,the accuracy=0.72677\n",
      "Under 3000 keyswords and in lift mode,the accuracy=0.69626\n",
      "Under 4000 keyswords and in lift mode,the accuracy=0.65049\n"
     ]
    }
   ],
   "source": [
    "def get_final_df(df, docs, all_df, all_docs,feature_count,usage):\n",
    "    \"\"\"combine the topic df and the total df and then calculate the chi square\"\"\"\n",
    "    all_df.columns = ['all_tf','all_df','all_tf-idf']\n",
    "    df = pd.merge(df, all_df, left_index = True, right_index = True, how = 'left')\n",
    "    df['midf'] = np.log(df.df/(df.all_df*docs))\n",
    "    df['tf_ev'] = df.all_tf/all_docs*docs\n",
    "    df['df_ev'] = df.all_df/all_docs*docs\n",
    "    df['tf_chi'] = ((df.tf-df.tf_ev)**2/df.tf_ev)*np.sign(df.tf-df.tf_ev)\n",
    "    df['df_chi'] = ((df.df-df.df_ev)**2/df.df_ev)*np.sign(df.df-df.df_ev)\n",
    "    df[\"lift\"]=(df.df/docs)/(df.all_df/all_docs)\n",
    "    df[\"mi*tfidf\"]=df[\"midf\"]*df[\"tfidf\"]\n",
    "   \n",
    "    df = df.sort_values(usage,ascending = False)[:feature_count]\n",
    "\n",
    "    return(df)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "for i in [\"mi*tfidf\",\"df_chi\",\"midf\",\"lift\"]:\n",
    "    for j in [1000,2000,3000,4000]:\n",
    "        fall=get_final_df(fall_all_gram,2543,data_all_gram,4806,j,i)\n",
    "        rise=get_final_df(rise_all_gram,2263,data_all_gram,4806,j,i)\n",
    "        drop=set(fall.index).intersection(rise.index)\n",
    "        fall=fall.drop(list(drop))\n",
    "        rise=rise.drop(list(drop))\n",
    "        total=list(fall.index)+list(rise.index)\n",
    "        feature=np.zeros((data.shape[0],len(total)))\n",
    "        for k in range(len(data)):\n",
    "            for l in range(len(total)):\n",
    "                if total[l] in data[\"all\"].iloc[k]:\n",
    "                    feature[k,l]+=1\n",
    "                else:\n",
    "                    feature[k,l]=feature[k,l]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(feature,data[\"tag\"], test_size=0.15, random_state=1,shuffle=True)#,stratify=data[\"tag\"])\n",
    "        logistic = LogisticRegression(random_state=0,max_iter=100000,C=0.1)\n",
    "        logistic.fit(x_train, y_train)\n",
    "        y_pred = logistic.predict(x_test)\n",
    "        print(\"Under %d keyswords and in %s mode,the accuracy=%.5f\" %(j,i,accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall=get_final_df(fall_all_gram,2543,data_all_gram,4806,1000,\"lift\")\n",
    "rise=get_final_df(rise_all_gram,2263,data_all_gram,4806,1000,\"lift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EBg3h1IwNncn"
   },
   "outputs": [],
   "source": [
    "drop=set(fall.index).intersection(rise.index)\n",
    "fall=fall.drop(list(drop))\n",
    "rise=rise.drop(list(drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Q_5GLdKhNncn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(list(fall.index)+list(rise.index)))\n",
    "total=list(fall.index)+list(rise.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=np.zeros((data.shape[0],len(total)))\n",
    "for k in range(len(data)):\n",
    "    for l in range(len(total)):\n",
    "        if total[l] in data[\"all\"].iloc[k]:\n",
    "            feature[k,l]+=1\n",
    "        else:\n",
    "            feature[k,l]=feature[k,l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "1kE7HnUVNncn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (forest): 0.87243\n",
      "Accuracy (forest): 0.74619\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature,data[\"tag\"], test_size=0.15, random_state=2,shuffle=True)\n",
    "\n",
    "forest = LogisticRegression(random_state=0,max_iter=100,C=0.19)\n",
    "forest.fit(x_train, y_train)\n",
    "y_pred = forest.predict(x_test)\n",
    "y_train_pred=forest.predict(x_train)\n",
    "print('Accuracy (forest): %.5f' % accuracy_score(y_train, y_train_pred))\n",
    "print('Accuracy (forest): %.5f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8DW2AJ2Nncn",
    "outputId": "5ae1777f-c902-483d-c536-eedabe126624"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9SkKiUwNncn",
    "outputId": "feaa901b-09bf-4a1f-d850-10d9a5887181"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_u0ypqONncn",
    "outputId": "14aa7695-46fa-4287-fd2b-5949b4a828e3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bA3464beNncn",
    "outputId": "47a92deb-7e82-449f-83e7-5b3b9719f2aa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZkm-mGGNncn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4M6F7YZNncn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "key_word_extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
