{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on word vectors\n",
    "\n",
    "Welcome to your first assignment of this week! \n",
    "\n",
    "Because word embeddings are very computationally expensive to train, most ML practitioners will load a pre-trained set of embeddings. \n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "\n",
    "- Load pre-trained word vectors, and measure similarity using cosine similarity\n",
    "- Use word embeddings to solve word analogy problems such as Man is to Woman as King is to ______. \n",
    "- Modify word embeddings to reduce their gender bias \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Updates</font>\n",
    "\n",
    "#### If you were working on the notebook before this update...\n",
    "* The current notebook is version \"2a\".\n",
    "* You can find your original work saved in the notebook with the previous version name (\"v2\") \n",
    "* To view the file directory, go to the menu \"File->Open\", and this will open a new tab that shows the file directory.\n",
    "\n",
    "#### List of updates\n",
    "* cosine_similarity\n",
    "    * Additional hints.\n",
    "* complete_analogy\n",
    "    * Replaces the list of input words with a set, and sets it outside the for loop (to follow best practices in coding).\n",
    "* Spelling, grammar and wording corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started! Run the following cell to load the packages you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from w2v_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the word vectors\n",
    "* For this assignment, we will use 50-dimensional GloVe vectors to represent words. \n",
    "* Run the following cell to load the `word_to_vec_map`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('../../readonly/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've loaded:\n",
    "- `words`: set of words in the vocabulary.\n",
    "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation.\n",
    "\n",
    "#### Embedding vectors versus one-hot vectors\n",
    "* Recall from the lesson videos that one-hot vectors do not do a good job of capturing the level of similarity between words (every one-hot vector has the same Euclidean distance from any other one-hot vector).\n",
    "* Embedding vectors such as GloVe vectors provide much more useful information about the meaning of individual words. \n",
    "* Lets now see how you can use GloVe vectors to measure the similarity between two words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Cosine similarity\n",
    "\n",
    "To measure the similarity between two words, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n",
    "\n",
    "* $u \\cdot v$ is the dot product (or inner product) of two vectors\n",
    "* $||u||_2$ is the norm (or length) of the vector $u$\n",
    "* $\\theta$ is the angle between $u$ and $v$. \n",
    "* The cosine similarity depends on the angle between $u$ and $v$. \n",
    "    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n",
    "    * If they are dissimilar, the cosine similarity will take a smaller value. \n",
    "\n",
    "<img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
    "<caption><center> **Figure 1**: The cosine of the angle between two vectors is a measure their similarity</center></caption>\n",
    "\n",
    "**Exercise**: Implement the function `cosine_similarity()` to evaluate the similarity between word vectors.\n",
    "\n",
    "**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$\n",
    "\n",
    "#### Additional Hints\n",
    "* You may find `np.dot`, `np.sum`, or `np.sqrt` useful depending upon the implementation that you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: cosine_similarity\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similarity between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0.0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Compute the dot product between u and v (≈1 line)\n",
    "    dot = np.dot(u,v)\n",
    "    # Compute the L2 norm of u (≈1 line)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    \n",
    "    # Compute the L2 norm of v (≈1 line)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    # Compute the cosine similarity defined by formula (1) (≈1 line)\n",
    "    cosine_similarity =dot/(norm_u*norm_v)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(father, mother) =  0.890903844289\n",
      "cosine_similarity(ball, crocodile) =  0.274392462614\n",
      "cosine_similarity(france - paris, rome - italy) =  -0.675147930817\n"
     ]
    }
   ],
   "source": [
    "father = word_to_vec_map[\"father\"]\n",
    "mother = word_to_vec_map[\"mother\"]\n",
    "ball = word_to_vec_map[\"ball\"]\n",
    "crocodile = word_to_vec_map[\"crocodile\"]\n",
    "france = word_to_vec_map[\"france\"]\n",
    "italy = word_to_vec_map[\"italy\"]\n",
    "paris = word_to_vec_map[\"paris\"]\n",
    "rome = word_to_vec_map[\"rome\"]\n",
    "\n",
    "print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\n",
    "print(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(father, mother)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.890903844289\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(ball, crocodile)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.274392462614\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(france - paris, rome - italy)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         -0.675147930817\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try different words!\n",
    "* After you get the correct expected output, please feel free to modify the inputs and measure the cosine similarity between other pairs of words! \n",
    "* Playing around with the cosine similarity of other inputs will give you a better sense of how word vectors behave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Word analogy task\n",
    "\n",
    "* In the word analogy task, we complete the sentence:  \n",
    "    <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. \n",
    "\n",
    "* An example is:  \n",
    "    <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. \n",
    "\n",
    "* We are trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner:   \n",
    "    $e_b - e_a \\approx e_d - e_c$\n",
    "* We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. \n",
    "\n",
    "**Exercise**: Complete the code below to be able to perform word analogies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: complete_analogy\n",
    "\n",
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_a -- a word, string\n",
    "    word_b -- a word, string\n",
    "    word_c -- a word, string\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert words to lowercase\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Get the word embeddings e_a, e_b and e_c (≈1-3 lines)\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a],word_to_vec_map[word_b],word_to_vec_map[word_c]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "\n",
    "    # to avoid best_word being one of the input words, skip the input words\n",
    "    # place the input words in a set for faster searching than a list\n",
    "    # We will re-use this set of input words inside the for-loop\n",
    "    input_words_set = set([word_a, word_b, word_c])\n",
    "    \n",
    "    # loop over the whole word vector set\n",
    "    for w in words:        \n",
    "        # to avoid best_word being one of the input words, skip the input words\n",
    "        if w in input_words_set:\n",
    "            continue\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
    "        cosine_sim = cosine_similarity(e_b-e_a,word_to_vec_map[w]-e_c )\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n",
    "        if cosine_sim >  max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your code, this may take 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: spain -> spanish\n",
      "india -> delhi :: japan -> tokyo\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: large -> larger\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **italy -> italian** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         spain -> spanish\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **india -> delhi** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         japan -> tokyo\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **man -> woman ** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         boy -> girl\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **small -> smaller ** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         large -> larger\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once you get the correct expected output, please feel free to modify the input cells above to test your own analogies. \n",
    "* Try to find some other analogy pairs that do work, but also find some where the algorithm doesn't give the right answer:\n",
    "    * For example, you can try small->smaller as big->?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "You've come to the end of the graded portion of the assignment. Here are the main points you should remember:\n",
    "\n",
    "- Cosine similarity is a good way to compare the similarity between pairs of word vectors.\n",
    "    - Note that L2 (Euclidean) distance also works.\n",
    "- For NLP applications, using a pre-trained set of word vectors is often a good way to get started.\n",
    "- Even though you have finished the graded portions, we recommend you take a look at the rest of this notebook to learn about debiasing word vectors.\n",
    "\n",
    "Congratulations on finishing the graded portions of this notebook! \n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "8hb5s",
   "launcher_item_id": "5NrJ6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
