{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "In this homework, we are going to play with Twitter data.\n",
    "\n",
    "The data is represented as rows of of [JSON](https://en.wikipedia.org/wiki/JSON#Example) strings.\n",
    "It consists of [tweets](https://dev.twitter.com/overview/api/tweets), [messages](https://dev.twitter.com/streaming/overview/messages-types), and a small amount of broken data (cannot be parsed as JSON).\n",
    "\n",
    "For this homework, we will only focus on tweets and ignore all other messages.\n",
    "\n",
    "\n",
    "## Tweets\n",
    "\n",
    "A tweet consists of many data fields. [Here is an example](https://gist.github.com/arapat/03d02c9b327e6ff3f6c3c5c602eeaf8b). You can learn all about them in the Twitter API doc. We are going to briefly introduce only the data fields that will be used in this homework.\n",
    "\n",
    "* `created_at`: Posted time of this tweet (time zone is included)\n",
    "* `id_str`: Tweet ID - we recommend using `id_str` over using `id` as Tweet IDs, becauase `id` is an integer and may bring some overflow problems.\n",
    "* `text`: Tweet content\n",
    "* `user`: A JSON object for information about the author of the tweet\n",
    "    * `id_str`: User ID\n",
    "    * `name`: User name (may contain spaces)\n",
    "    * `screen_name`: User screen name (no spaces)\n",
    "* `retweeted_status`: A JSON object for information about the retweeted tweet (i.e. this tweet is not original but retweeteed some other tweet)\n",
    "    * All data fields of a tweet except `retweeted_status`\n",
    "* `entities`: A JSON object for all entities in this tweet\n",
    "    * `hashtags`: An array for all the hashtags that are mentioned in this tweet\n",
    "    * `urls`: An array for all the URLs that are mentioned in this tweet\n",
    "\n",
    "\n",
    "## Data source\n",
    "\n",
    "All tweets are collected using the [Twitter Streaming API](https://dev.twitter.com/streaming/overview).\n",
    "\n",
    "\n",
    "## Users partition\n",
    "\n",
    "Besides the original tweets, we will provide you with a Pickle file, which contains a partition over 452,743 Twitter users. It contains a Python dictionary `{user_id: partition_id}`. The users are partitioned into 7 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can load pickle_content from a file on the local file system\n",
    "# while testing on your laptop\n",
    "# To test on your laptop, set `ON_EMR=False`\n",
    "# To test on AWS for final submission, set `ON_EMR=True`\n",
    "\n",
    "ON_EMR = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "We ask you use the `OutputLogger` object `my_output` to store the\n",
    "results of your program.\n",
    "We have provided function calls to `my_output.append()` method for\n",
    "storing the results in all necessary places.\n",
    "Please make sure NOT to remove these lines\n",
    "\n",
    "In the last cell of this file, we write the content of `my_output`\n",
    "to a pickle file which the grader will read in and use for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "class OutputLogger:\n",
    "    def __init__(self):\n",
    "        self.ans = {}\n",
    "\n",
    "    def append(self, key, value):\n",
    "        self.ans[key] = value\n",
    "\n",
    "    def write_to_disk(self):\n",
    "        if ON_EMR:\n",
    "            filepath = os.path.expanduser(\"answer.pickle\")\n",
    "            print(\"FilePath = {}\".format(filepath))\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(self.ans, f)\n",
    "            proc = subprocess.Popen([\"/usr/local/hadoop/hadoop-2.7.4/bin/hadoop\", \"fs\", \"-copyFromLocal\", filepath, \"/user/spark/answer.pickle\"])\n",
    "            proc.wait()\n",
    "            os.remove(filepath)\n",
    "        else:\n",
    "            filepath = os.path.expanduser(\"~/answer.pickle\")\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(self.ans, f)\n",
    "\n",
    "\n",
    "my_output = OutputLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Load data to a RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets data is stored on AWS S3. We have in total a little over 1 TB of tweets. We provide 10 MB of tweets for your local development. For the testing and grading on the homework server, we will use different data.\n",
    "\n",
    "## Testing on the homework server\n",
    "On EdX, we provide three different input sizes to test your program: 10 MB, 1 GB, and 10 GB. For any run, we will only be using one of these four datasets.\n",
    "\n",
    "For submission and for local testing, make sure to read the path of the file you want to operate with from `./hw2-files.txt`. Otherwise your program will receive no points.\n",
    "\n",
    "## Local test\n",
    "\n",
    "For local testing, please create your own `hw2-files.txt` file, which contains a single file path on the local disk, e.g.\n",
    "`file://<absolute_path_to_current_directory>/hw2-files-10mb.txt`. For final submission, we will create this file on our server for testing with the appropriate file path. If your implementation is correct, you should not worry about which file system (i.e. local file system or HDFS) Spark will read data from.\n",
    "\n",
    "Now let's see how many lines there are in the input files.\n",
    "\n",
    "1. Make RDD from the data in the file given by the file path present in `hw2-files.txt`.\n",
    "2. Mark the RDD to be cached (so in next operation data will be loaded in memory) \n",
    "3. call the `count` method to print number of lines in all these files\n",
    "\n",
    "<b>It should print<b>\n",
    "```\n",
    "Number of elements: 2150\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a useful cell for debugging.\n",
    "Use timer_start() and timer_stop() at different parts of your code\n",
    "for checking the amount of time a segment takes.\n",
    "\"\"\"\n",
    "from time import time\n",
    "\n",
    "\n",
    "timer = []\n",
    "prev_ts = None\n",
    "\n",
    "\n",
    "def timer_start():\n",
    "    global prev_ts\n",
    "    prev_ts = time()\n",
    "\n",
    "\n",
    "def timer_stop(title):\n",
    "    timer.append((title, time() - prev_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer_start()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "timer_stop(\"set up sc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements: 2150\n"
     ]
    }
   ],
   "source": [
    "timer_start()\n",
    "\n",
    "\n",
    "with open('./hw2-files.txt') as f:\n",
    "    file_path = [w.strip() for w in f.readlines() if w.strip()]\n",
    "\n",
    "# Your code here\n",
    "data=sc.textFile(\"hw2-files-10mb.txt\").cache()\n",
    "count=sc.textFile(\"hw2-files-10mb.txt\").count()\n",
    "\n",
    "my_output.append(\"num-tweets\", count)\n",
    "print('Number of elements:', count)\n",
    "timer_stop(\"read data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Parse JSON strings to JSON objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has built-in support for JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1, 'name': 'A green door', 'price': 12.5, 'tags': ['home', 'green']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_example = '''\n",
    "{\n",
    "    \"id\": 1,\n",
    "    \"name\": \"A green door\",\n",
    "    \"price\": 12.50,\n",
    "    \"tags\": [\"home\", \"green\"]\n",
    "}\n",
    "'''\n",
    "\n",
    "json_obj = json.loads(json_example)\n",
    "json_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broken tweets and irrelevant messages\n",
    "\n",
    "The data of this assignment may contain broken tweets (invalid JSON strings). So make sure that your code is robust for such cases.\n",
    "\n",
    "You can filter out such broken tweet by checking if:\n",
    "* the line is not in json format\n",
    "\n",
    "In addition, some lines in the input file might not be tweets, but messages that the Twitter server sent to the developer (such as [limit notices](https://dev.twitter.com/streaming/overview/messages-types#limit_notices)). Your program should also ignore these messages.\n",
    "\n",
    "These messages would not contain the `created_at` field and can be filtered out accordingly.\n",
    "* Check if json object of the broken tweet has a `created_at` field\n",
    "\n",
    "*Hint:* [Catch the ValueError](http://stackoverflow.com/questions/11294535/verify-if-a-string-is-json-in-python)\n",
    "\n",
    "**********************************************************************************\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "(1) Parse raw JSON tweets to obtain valid JSON objects. \n",
    "\n",
    "(2) From all valid tweets, construct a pair RDD of `(user_id, text)`, where `user_id` is the `id_str` data field of the `user` dictionary (read [Tweets](#Tweets) section above), `text` is the `text` data field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def safe_parse(raw_json):\n",
    "    \"\"\"\n",
    "    Input is a String\n",
    "    Output is a JSON object if the tweet is valid and None if not valid\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    try:\n",
    "        json_object = json.loads(raw_json)\n",
    "        if \"created_at\" not in json_object.keys():\n",
    "            return None\n",
    "        else :\n",
    "            return  (json_object[\"user\"][\"id_str\"],json_object[\"text\"])\n",
    "    except ValueError as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('470520068', \"I'm voting 4 #BernieSanders bc he doesn't ride a CAPITALIST PIG adorned w/ #GoldmanSachs $. SYSTEM RIGGED CLASS WAR https://t.co/P7pFm2MT9e\"), ('2176120173', \"RT @TrumpNewMedia: .@realDonaldTrump #America get out &amp; #VoteTrump if you don't #VoteTrump NOTHING will change it's that simple!\\n#Trump htt…\"), ('145087572', 'RT @Libertea2012: RT TODAY: #Colorado’s leading progressive voices to endorse @BernieSanders! #Denver 11AM - 1PM in MST CO State Capitol…')]\n",
      "2133\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Remember to construct an RDD of (user_id, text) here.\n",
    "\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "data=data.map(lambda x:safe_parse(x)).filter(lambda x:x!=None)\n",
    "print(data.take(3))\n",
    "print(data.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of unique users\n",
    "\n",
    "Count the number of different users in all valid tweets\n",
    "\n",
    "(hint: [the `distinct()` method](https://spark.apache.org/docs/latest/programming-guide.html#transformations) is an easy way to do this, but try to see if there is a faster way to do this).\n",
    "\n",
    "*******************************************************************************\n",
    "\n",
    "**It should print**\n",
    "```\n",
    "The number of unique users is: 1748\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique users is: 1748\n"
     ]
    }
   ],
   "source": [
    "timer_start()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "temp=data.groupByKey()\n",
    "users_count=temp.count()\n",
    "\n",
    "my_output.append(\"num-unique-users\", users_count)\n",
    "print('The number of unique users is:', users_count)\n",
    "timer_stop(\"Count unique users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Number of posts from each user partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Pickle file `/twitter/users-partition.pickle`, you will get a dictionary which represents a partition over 452,743 Twitter users, `{user_id: partition_id}`. The users are partitioned into 7 groups. For example, if the dictionary is loaded into a variable named `partition`, the partition ID of the user `59458445` is `partition[\"59458445\"]`. These users are partitioned into 7 groups. The partition ID is an integer between 0-6.\n",
    "\n",
    "Note that the user partition we provide doesn't cover all users appear in the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pickle file\n",
    "\n",
    "For local testing, you can load the pickle file from the local file system, namely\n",
    "\n",
    "```\n",
    "proc = subprocess.Popen([\"cat\", \"./users-partition.pickle\"],\n",
    "                        stdout=subprocess.PIPE)\n",
    "pickle_content = proc.communicate()[0]\n",
    "```\n",
    "However, for submission, please keep following code block unchanged, since on the server the pickle file is located on the HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452743"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import pickle\n",
    "\n",
    "if ON_EMR:\n",
    "    proc = subprocess.Popen([\"/usr/local/hadoop/hadoop-2.7.4/bin/hadoop\", \"fs\", \"-cat\", \"/user/spark/twitter/users-partition.pickle\"],\n",
    "                            stdout=subprocess.PIPE)\n",
    "    pickle_content = proc.communicate()[0]\n",
    "    \n",
    "else:\n",
    "    #!wget 'http://mas-dse-open.s3.amazonaws.com/Twitter/users-partition.pickle' -O './users-partition.pickle'\n",
    "    proc = subprocess.Popen([\"cat\", \"./users-partition.pickle\"],\n",
    "                        stdout=subprocess.PIPE)\n",
    "    pickle_content = proc.communicate()[0]\n",
    "\n",
    "partition = pickle.loads(pickle_content)\n",
    "len(partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets per user partition\n",
    "\n",
    "1. Count the number of posts from each user partition\n",
    "\n",
    "2. Count the number of posts from group 0, 1, ..., 6, plus the number of posts from users who are not in any partition. Assign users who are not in any partition to the group 7.\n",
    "\n",
    "3. Put the results of this step into a pair RDD `(group_id, count)` that is sorted by key.\n",
    "\n",
    "\n",
    "\n",
    "Print the post count using the `print_post_count` function we provided.\n",
    "\n",
    "**It should print**\n",
    "\n",
    "```\n",
    "Group 0 posted 87 tweets\n",
    "Group 1 posted 242 tweets\n",
    "Group 2 posted 41 tweets\n",
    "Group 3 posted 349 tweets\n",
    "Group 4 posted 101 tweets\n",
    "Group 5 posted 358 tweets\n",
    "Group 6 posted 434 tweets\n",
    "Group 7 posted 521 tweets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2133\n",
      "[('3252778713', (\"@SurfPHX \\n\\nDems give illegals Immigrants more Gov entitlements then American CitizensWhoPayTaxes!The so called Republicans,don't even fight.\", 6)), ('754243238', ('RT @Adenovir: If Donald Trump is elected, Canada will build a wall and America will pay for it. https://t.co/aZ0iXTZ2nW', 1)), ('594156339', ('In one week, \"Super Tuesday\" begins, and intrigue really on GOP side because delegates awarded proportionally--if you meet minimum support.', None))]\n"
     ]
    }
   ],
   "source": [
    "tweet=sc.parallelize(partition.items())\n",
    "data=data.leftOuterJoin(tweet)\n",
    "\n",
    "print(len(data.collect()))\n",
    "print(data.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, '3252778713'),\n",
       " (1, '754243238'),\n",
       " (7, '594156339'),\n",
       " (1, '3516909376'),\n",
       " (5, '432292106')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def change(x):\n",
    "    if x==None:\n",
    "        return 7\n",
    "    else:\n",
    "        return x\n",
    "tweet=data.map(lambda x:(change(x[1][1]),x[0]))\n",
    "tweet.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 87), (1, 242), (2, 41), (3, 349), (4, 101), (5, 358), (6, 434), (7, 521)]\n",
      "Group 0 posted 87 tweets\n",
      "Group 1 posted 242 tweets\n",
      "Group 2 posted 41 tweets\n",
      "Group 3 posted 349 tweets\n",
      "Group 4 posted 101 tweets\n",
      "Group 5 posted 358 tweets\n",
      "Group 6 posted 434 tweets\n",
      "Group 7 posted 521 tweets\n",
      "2133\n"
     ]
    }
   ],
   "source": [
    "def print_post_count(counts):\n",
    "    for group_id, count in counts:\n",
    "        print('Group %d posted %d tweets' % (group_id, count))\n",
    "counts=tweet.groupByKey().mapValues(lambda x: len(list(x)))\n",
    "print(counts.collect())\n",
    "print_post_count(counts.collect())\n",
    "print(tweet.map(lambda x:(x[1],x[0])).groupByKey().mapValues(lambda x: len(list(x))).map(lambda x:x[1]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer_start()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "counts_per_partition=counts.collect()\n",
    "\n",
    "# Following code adds your solution to `my_output`\n",
    "assert(type(counts_per_partition) is list and\n",
    "       len(counts_per_partition) == 8 and\n",
    "       len(counts_per_partition[0]) == 2)\n",
    "my_output.append(\"counts_per_part\", counts_per_partition)\n",
    "timer_stop(\"Count tweets per user partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 3:  Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a tweet tokenizer for you in the following cells. This Tokenizer object is called `tok`. Don't forget to execute the two cells below.\n",
    "\n",
    "You can expand the following cell if needed to see the minutae of the Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "This code implements a basic, Twitter-aware tokenizer.\n",
    "\n",
    "A tokenizer is a function that splits a string of text into words. In\n",
    "Python terms, we map string and unicode objects into lists of unicode\n",
    "objects.\n",
    "\n",
    "There is not a single right way to do tokenizing. The best method\n",
    "depends on the application.  This tokenizer is designed to be flexible\n",
    "and this easy to adapt to new domains and tasks.  The basic logic is\n",
    "this:\n",
    "\n",
    "1. The tuple regex_strings defines a list of regular expression\n",
    "   strings.\n",
    "\n",
    "2. The regex_strings strings are put, in order, into a compiled\n",
    "   regular expression object called word_re.\n",
    "\n",
    "3. The tokenization is done by word_re.findall(s), where s is the\n",
    "   user-supplied string, inside the tokenize() method of the class\n",
    "   Tokenizer.\n",
    "\n",
    "4. When instantiating Tokenizer objects, there is a single option:\n",
    "   preserve_case.  By default, it is set to True. If it is set to\n",
    "   False, then the tokenizer will downcase everything except for\n",
    "   emoticons.\n",
    "\n",
    "The __main__ method illustrates by tokenizing a few examples.\n",
    "\n",
    "I've also included a Tokenizer method tokenize_random_tweet(). If the\n",
    "twitter library is installed (http://code.google.com/p/python-twitter/)\n",
    "and Twitter is cooperating, then it should tokenize a random\n",
    "English-language tweet.\n",
    "\n",
    "\n",
    "Julaiti Alafate:\n",
    "  I modified the regex strings to extract URLs in tweets.\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"Christopher Potts\"\n",
    "__copyright__ = \"Copyright 2011, Christopher Potts\"\n",
    "__credits__ = []\n",
    "__license__ = \"Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-nc-sa/3.0/\"\n",
    "__version__ = \"1.0\"\n",
    "__maintainer__ = \"Christopher Potts\"\n",
    "__email__ = \"See the author's website\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "import re\n",
    "from html import entities \n",
    "\n",
    "######################################################################\n",
    "# The following strings are components in the regular expression\n",
    "# that is used for tokenizing. It's important that phone_number\n",
    "# appears first in the final regex (since it can contain whitespace).\n",
    "# It also could matter that tags comes after emoticons, due to the\n",
    "# possibility of having text like\n",
    "#\n",
    "#     <:| and some text >:)\n",
    "#\n",
    "# Most imporatantly, the final element should always be last, since it\n",
    "# does a last ditch whitespace-based tokenization of whatever is left.\n",
    "\n",
    "# This particular element is used in a couple ways, so we define it\n",
    "# with a name:\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?            \n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?    \n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*   \n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # URLs:\n",
    "    r\"\"\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,    \n",
    "    # HTML tags:\n",
    "     r\"\"\"<[^>]+>\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots. \n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "######################################################################\n",
    "# This is the core tokenizing regex:\n",
    "    \n",
    "word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# These are for regularizing HTML entities to Unicode:\n",
    "html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
    "html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
    "amp = \"&amp;\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, preserve_case=False):\n",
    "        self.preserve_case = preserve_case\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        \"\"\"\n",
    "        Argument: s -- any string or unicode object\n",
    "        Value: a tokenize list of strings; conatenating this list returns the original string if preserve_case=False\n",
    "        \"\"\"        \n",
    "        # Try to ensure unicode:\n",
    "        try:\n",
    "            s = str(s)\n",
    "        except UnicodeDecodeError:\n",
    "            s = s.encode('string_escape')\n",
    "            s = str(s)\n",
    "        # Fix HTML character entitites:\n",
    "        s = self.__html2unicode(s)\n",
    "        # Tokenize:\n",
    "        words = word_re.findall(s)\n",
    "        # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
    "        if not self.preserve_case:            \n",
    "            words = map((lambda x : x if emoticon_re.search(x) else x.lower()), words)\n",
    "        return words\n",
    "\n",
    "    def tokenize_random_tweet(self):\n",
    "        \"\"\"\n",
    "        If the twitter library is installed and a twitter connection\n",
    "        can be established, then tokenize a random tweet.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import twitter\n",
    "        except ImportError:\n",
    "            print(\"Apologies. The random tweet functionality requires the Python twitter library: http://code.google.com/p/python-twitter/\")\n",
    "        from random import shuffle\n",
    "        api = twitter.Api()\n",
    "        tweets = api.GetPublicTimeline()\n",
    "        if tweets:\n",
    "            for tweet in tweets:\n",
    "                if tweet.user.lang == 'en':            \n",
    "                    return self.tokenize(tweet.text)\n",
    "        else:\n",
    "            raise Exception(\"Apologies. I couldn't get Twitter to give me a public English-language tweet. Perhaps try again\")\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))\t\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x : x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:            \n",
    "                s = s.replace(ent, unichr(entities.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass                    \n",
    "            s = s.replace(amp, \" and \")\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "tok = Tokenizer(preserve_case=False)\n",
    "\n",
    "\n",
    "def print_tokens(tokens, gid = None):\n",
    "    group_name = \"overall\"\n",
    "    if gid is not None:\n",
    "        group_name = \"group %d\" % gid\n",
    "    print('=' * 5 + ' ' + group_name + ' ' + '=' * 5)\n",
    "    for t, n in tokens:\n",
    "        print(\"%s\\t%.4f\" % (t, n))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize tweets\n",
    "\n",
    "1. Tokenize the tweets using the `tokenize` function that is a method of the `Tokenizer` class that we have instantiated as `tok`. \n",
    "\n",
    "1. Count the number of mentions for each tokens regardless of specific user group.\n",
    "\n",
    "1. Call `print_count` function to show how many different tokens we have.\n",
    "\n",
    "**It should print**\n",
    "```\n",
    "Number of tokens: 7677\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 7677\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "tweet=data.map(lambda x:(change(x[1][1]),x[1][0],x[0]))\n",
    "tweet.take(3)\n",
    "import itertools\n",
    "answer=list(itertools.chain(*tweet.map(lambda x:list(tok.tokenize(x[1]))).collect()))\n",
    "num_of_tokens=len(set(answer))\n",
    "\n",
    "my_output.append(\"num-tokens\", num_of_tokens)\n",
    "print(\"Number of tokens:\", num_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token popularity\n",
    "\n",
    "Tokens that are mentioned by too few users are usually not very interesting. So we want to only keep tokens that are mentioned by at least 100 users. Filter out tokens that don't meet this requirement.\n",
    "\n",
    "Call `print_count` function to show how many different tokens we have after the filtering.\n",
    "\n",
    "Call `print_tokens` function to show top 20 most frequent tokens.\n",
    "\n",
    "**It should print**\n",
    "```\n",
    "Number of tokens: 46\n",
    "===== overall =====\n",
    ":\t1046.0000\n",
    "rt\t920.0000\n",
    ".\t767.0000\n",
    "the\t587.0000\n",
    "trump\t560.0000\n",
    "…\t520.0000\n",
    "to\t501.0000\n",
    ",\t497.0000\n",
    "in\t385.0000\n",
    "a\t383.0000\n",
    "is\t382.0000\n",
    "of\t300.0000\n",
    "!\t285.0000\n",
    "for\t275.0000\n",
    "and\t263.0000\n",
    "on\t218.0000\n",
    "i\t216.0000\n",
    "he\t191.0000\n",
    "that\t190.0000\n",
    "\"\t181.0000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 7, 3, 1, 2, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 29, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1]\n",
      "['3252778713', '754243238', '594156339']\n",
      "RT @izstumtais: Mani draugi kādreiz spēlēja grupā Smilšu vētra jeb Sandstorm, viņiem bija tādi hīti kā \"Kamielīt, kamielīt saki man tā\" un …\n",
      "['RT @izstumtais: Mani draugi kādreiz spēlēja grupā Smilšu vētra jeb Sandstorm, viņiem bija tādi hīti kā \"Kamielīt, kamielīt saki man tā\" un …']\n",
      "['rt', '@izstumtais', ':', 'mani', 'draugi', 'kādreiz', 'spēlēja', 'grup', 'ā', 'smil', 'šu', 'vētra', 'jeb', 'sandstorm', ',', 'viņiem', 'bija', 'tādi', 'hīti', 'kā', '\"', 'kamiel', 'īt', ',', 'kamiel', 'īt', 'saki', 'man', 'tā', '\"', 'un', '…']\n"
     ]
    }
   ],
   "source": [
    "a=tweet.map(lambda x:(x[2],x[1])).groupByKey().mapValues(lambda x: \" \".join(x))\n",
    "b=tweet.map(lambda x:(x[2],x[1])).groupByKey().mapValues(lambda x: list(x))\n",
    "print(b.map(lambda x:len(x[1])).take(100))\n",
    "print(a.keys().take(3))\n",
    "print(a.take(100)[7][1])\n",
    "print(b.take(100)[7][1])\n",
    "print(list(tok.tokenize(a.take(100)[7][1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(':', 1046),\n",
       " ('rt', 920),\n",
       " ('.', 767),\n",
       " ('the', 587),\n",
       " ('trump', 560),\n",
       " ('…', 520),\n",
       " ('to', 501),\n",
       " (',', 497),\n",
       " ('in', 385),\n",
       " ('a', 383),\n",
       " ('is', 382),\n",
       " ('of', 300),\n",
       " ('!', 285),\n",
       " ('for', 275),\n",
       " ('and', 263),\n",
       " ('on', 218),\n",
       " ('i', 216),\n",
       " ('he', 191),\n",
       " ('that', 190),\n",
       " ('\"', 181),\n",
       " ('you', 169),\n",
       " ('donald', 160),\n",
       " ('-', 160),\n",
       " ('?', 155),\n",
       " ('bernie', 151),\n",
       " ('not', 143),\n",
       " ('will', 141),\n",
       " ('hillary', 135),\n",
       " (';', 134),\n",
       " ('this', 129),\n",
       " ('cruz', 126),\n",
       " ('with', 122),\n",
       " ('are', 119),\n",
       " ('&', 119),\n",
       " ('be', 118),\n",
       " ('sanders', 117),\n",
       " ('it', 116),\n",
       " ('have', 111),\n",
       " ('’', 109),\n",
       " ('https', 108),\n",
       " ('what', 108),\n",
       " ('about', 107),\n",
       " ('gop', 106),\n",
       " ('...', 104),\n",
       " ('amp', 102),\n",
       " ('his', 101)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "ans=dict()\n",
    "for i in tweet.map(lambda x:(x[2],x[1])).groupByKey().mapValues(lambda x: \" \".join(x)).map(lambda x:set(list(tok.tokenize(x[1])))).collect():\n",
    "    for j in i:\n",
    "        if j not in ans:\n",
    "            ans[j]=1\n",
    "        else:\n",
    "            ans[j]+=1\n",
    "\n",
    "res = {k: v for k, v in ans.items() if v>=100}\n",
    "sorted_keys = sorted(res.items(),key=operator.itemgetter(1),reverse=True)\n",
    "print(len(sorted_keys))\n",
    "sorted_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 46\n",
      "===== overall =====\n",
      ":\t1046.0000\n",
      "rt\t920.0000\n",
      ".\t767.0000\n",
      "the\t587.0000\n",
      "trump\t560.0000\n",
      "…\t520.0000\n",
      "to\t501.0000\n",
      ",\t497.0000\n",
      "in\t385.0000\n",
      "a\t383.0000\n",
      "is\t382.0000\n",
      "of\t300.0000\n",
      "!\t285.0000\n",
      "for\t275.0000\n",
      "and\t263.0000\n",
      "on\t218.0000\n",
      "i\t216.0000\n",
      "he\t191.0000\n",
      "that\t190.0000\n",
      "\"\t181.0000\n"
     ]
    }
   ],
   "source": [
    "timer_start()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "num_freq_tokens=len(sorted_keys)\n",
    "top20=sorted_keys[:20]\n",
    "\n",
    "my_output.append(\"num-freq-tokens\", num_freq_tokens)\n",
    "my_output.append(\"top-20-tokens\", top20)\n",
    "print(\"Number of tokens:\", num_freq_tokens)\n",
    "print_tokens(top20)\n",
    "timer_stop(\"Count overall most popular tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('594156339', ({',', 'delegates', 'tuesday', '\"', 'super', 'on', 'in', 'because', 'proportionally--if', 'one', '.', 'week', 'really', 'begins', 'intrigue', 'gop', 'awarded', 'you', 'side', 'minimum', 'support', 'meet', 'and'}, 7))]\n",
      "[(7, {',', 'delegates', 'tuesday', '\"', 'super', 'on', 'in', 'because', 'proportionally--if', 'one', '.', 'week', 'really', 'begins', 'intrigue', 'gop', 'awarded', 'you', 'side', 'minimum', 'support', 'meet', 'and'})]\n",
      "[('594156339', {',', 'delegates', 'tuesday', '\"', 'super', 'on', 'in', 'because', 'proportionally--if', 'one', '.', 'week', 'really', 'begins', 'intrigue', 'gop', 'awarded', 'you', 'side', 'minimum', 'support', 'meet', 'and'})]\n",
      "[('594156339', 7)]\n",
      "[('594156339', ({',', 'delegates', 'tuesday', '\"', 'super', 'on', 'in', 'because', 'proportionally--if', 'one', '.', 'week', 'really', 'begins', 'intrigue', 'gop', 'awarded', 'you', 'side', 'minimum', 'support', 'meet', 'and'}, 7))]\n"
     ]
    }
   ],
   "source": [
    "a1= tweet.map(lambda x:(x[2],x[1])).groupByKey().map(lambda x: (x[0],\" \".join(x[1]))).map(lambda x:(x[0],set(list(tok.tokenize(x[1])))))\n",
    "a2=tweet.map(lambda x:(x[2],x[0]))\n",
    "a3=a1.leftOuterJoin(a2)\n",
    "print(a3.take(1))\n",
    "print(a3.map(lambda x:(x[1][1],x[1][0])).take(1))\n",
    "print(a3.mapValues(lambda x:x[0]).take(1))\n",
    "print(a3.mapValues(lambda x:x[1]).take(1))\n",
    "print(a3.mapValues(lambda x:(x[0],x[1])).take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
