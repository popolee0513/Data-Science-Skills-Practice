{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencing Modeling\n",
    "\n",
    "> In this post, it will cover various network architectures and layers that we can use to make predictions from sequence data. This is the summary of lecture \"Customizing your model with Tensorflow 2\" from Coursera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow : v2.3.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "print(\"Tensorflow : v\"+tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  4, 12, 33, 18],\n",
       "       [63, 23, 54, 30, 19,  3],\n",
       "       [ 0, 43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "test_data = [\n",
    "    [4, 12, 33, 18],\n",
    "    [63, 23, 54, 30, 19, 3],\n",
    "    [43, 37, 11, 33, 15]\n",
    "]\n",
    "\n",
    "preprocessed_data = pad_sequences(test_data, padding='pre')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### post padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18,  0,  0],\n",
       "       [63, 23, 54, 30, 19,  3],\n",
       "       [43, 37, 11, 33, 15,  0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post padding with maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18,  0],\n",
       "       [23, 54, 30, 19,  3],\n",
       "       [43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post', maxlen=5)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post padding with truncating (Default: 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18,  0],\n",
       "       [63, 23, 54, 30, 19],\n",
       "       [43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post', maxlen=5, truncating='post')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post padding with truncating, then filled with value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 33, 18, -1],\n",
       "       [63, 23, 54, 30, 19],\n",
       "       [43, 37, 11, 33, 15]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = pad_sequences(test_data, padding='post', maxlen=5, truncating='post', value=-1)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example in 2d array sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2, 1],\n",
       "        [3, 3],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[4, 3],\n",
       "        [2, 4],\n",
       "        [1, 1]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = [\n",
    "    [[2, 1], [3, 3]],\n",
    "    [[4, 3], [2, 4], [1, 1]]\n",
    "]\n",
    "\n",
    "preprocessed_data = pad_sequences(test_input, padding='post')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking Layer for mask specific sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6, 1), dtype=int32, numpy=\n",
       "array([[[ 4],\n",
       "        [12],\n",
       "        [33],\n",
       "        [18],\n",
       "        [ 0],\n",
       "        [ 0]],\n",
       "\n",
       "       [[63],\n",
       "        [23],\n",
       "        [54],\n",
       "        [30],\n",
       "        [19],\n",
       "        [ 3]],\n",
       "\n",
       "       [[43],\n",
       "        [37],\n",
       "        [11],\n",
       "        [33],\n",
       "        [15],\n",
       "        [ 0]]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Masking\n",
    "\n",
    "preprocessed_data = pad_sequences(test_data, padding='post')\n",
    "\n",
    "masking_layer = Masking(mask_value=0)\n",
    "preprocessed_data = preprocessed_data[..., tf.newaxis] # (batch_size, seq_length, features)\n",
    "masked_input = masking_layer(preprocessed_data)\n",
    "masked_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True, False]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_input._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset with different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "         list([1, 194, 2, 194, 2, 78, 228, 5, 6, 2, 2, 2, 134, 26, 4, 2, 8, 118, 2, 14, 394, 20, 13, 119, 2, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2, 2, 5, 2, 4, 116, 9, 35, 2, 4, 229, 9, 340, 2, 4, 118, 9, 4, 130, 2, 19, 4, 2, 5, 89, 29, 2, 46, 37, 4, 455, 9, 45, 43, 38, 2, 2, 398, 4, 2, 26, 2, 5, 163, 11, 2, 2, 4, 2, 9, 194, 2, 7, 2, 2, 349, 2, 148, 2, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 2, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 2, 245, 2, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 9, 6, 371, 78, 22, 2, 64, 2, 9, 8, 168, 145, 23, 4, 2, 15, 16, 4, 2, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 4, 86, 320, 35, 2, 19, 263, 2, 2, 4, 2, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 2, 43, 2, 2, 8, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 2, 8, 106, 14, 2, 2, 18, 6, 22, 12, 215, 28, 2, 40, 6, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2, 51, 9, 170, 23, 2, 116, 2, 2, 13, 191, 79, 2, 89, 2, 14, 9, 8, 106, 2, 2, 35, 2, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 2, 9, 6, 2, 446, 2, 45, 2, 84, 2, 2, 21, 4, 2, 84, 2, 325, 2, 134, 2, 2, 84, 5, 36, 28, 57, 2, 21, 8, 140, 8, 2, 5, 2, 84, 56, 18, 2, 14, 9, 31, 7, 4, 2, 2, 2, 2, 2, 18, 6, 20, 207, 110, 2, 12, 8, 2, 2, 8, 97, 6, 20, 53, 2, 74, 4, 460, 364, 2, 29, 270, 11, 2, 108, 45, 40, 29, 2, 395, 11, 6, 2, 2, 7, 2, 89, 364, 70, 29, 140, 4, 64, 2, 11, 4, 2, 26, 178, 4, 2, 443, 2, 5, 27, 2, 117, 2, 2, 165, 47, 84, 37, 131, 2, 14, 2, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 4, 65, 496, 4, 231, 7, 2, 5, 6, 320, 234, 2, 234, 2, 2, 7, 496, 4, 139, 2, 2, 2, 2, 5, 2, 18, 4, 2, 2, 250, 11, 2, 2, 4, 2, 2, 2, 2, 372, 2, 2, 2, 2, 7, 4, 59, 2, 4, 2, 2]),\n",
       "         list([1, 2, 2, 69, 72, 2, 13, 2, 2, 8, 12, 2, 23, 5, 16, 484, 2, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 2, 51, 2, 32, 61, 369, 71, 66, 2, 12, 2, 75, 100, 2, 8, 4, 105, 37, 69, 147, 2, 75, 2, 44, 257, 390, 5, 69, 263, 2, 105, 50, 286, 2, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 2, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 4, 2, 2, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 2, 25, 8, 2, 12, 145, 5, 202, 12, 160, 2, 202, 12, 6, 52, 58, 2, 92, 401, 2, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 2, 2, 101, 405, 39, 14, 2, 4, 2, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 2, 102, 7, 4, 2, 2, 9, 24, 6, 78, 2, 17, 2, 2, 21, 27, 2, 2, 5, 2, 2, 92, 2, 4, 2, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 2, 4, 2, 2, 5, 2, 272, 191, 2, 6, 2, 8, 2, 2, 2, 2, 5, 383, 2, 2, 2, 2, 497, 2, 8, 2, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 2, 40, 4, 248, 20, 12, 16, 5, 174, 2, 72, 7, 51, 6, 2, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 2, 202, 14, 31, 6, 2, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 2]),\n",
       "         list([1, 14, 22, 2, 6, 176, 7, 2, 88, 12, 2, 23, 2, 5, 109, 2, 4, 114, 9, 55, 2, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 2, 2, 4, 2, 2, 109, 2, 21, 4, 22, 2, 8, 6, 2, 2, 10, 10, 4, 105, 2, 35, 2, 2, 19, 2, 2, 5, 2, 2, 45, 55, 221, 15, 2, 2, 2, 14, 2, 4, 405, 5, 2, 7, 27, 85, 108, 131, 4, 2, 2, 2, 405, 9, 2, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 2, 239, 34, 2, 2, 45, 407, 31, 7, 41, 2, 105, 21, 59, 299, 12, 38, 2, 5, 2, 15, 45, 2, 488, 2, 127, 6, 52, 292, 17, 4, 2, 185, 132, 2, 2, 2, 488, 2, 47, 6, 392, 173, 4, 2, 2, 270, 2, 4, 2, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 2, 2, 7, 2, 2, 2, 5, 2, 30, 2, 2, 56, 4, 2, 5, 2, 2, 8, 4, 2, 398, 229, 10, 10, 13, 2, 2, 2, 14, 9, 31, 7, 27, 111, 108, 15, 2, 19, 2, 2, 2, 2, 14, 22, 9, 2, 21, 45, 2, 5, 45, 252, 8, 2, 6, 2, 2, 2, 39, 4, 2, 48, 25, 181, 8, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 2, 8, 169, 11, 374, 2, 25, 203, 28, 8, 2, 12, 125, 4, 2]),\n",
       "         list([1, 111, 2, 2, 2, 2, 2, 4, 87, 2, 2, 7, 31, 318, 2, 7, 4, 498, 2, 2, 63, 29, 2, 220, 2, 2, 5, 17, 12, 2, 220, 2, 17, 6, 185, 132, 2, 16, 53, 2, 11, 2, 74, 4, 438, 21, 27, 2, 2, 8, 22, 107, 2, 2, 2, 2, 8, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 5, 2, 98, 31, 2, 33, 6, 58, 14, 2, 2, 8, 4, 365, 7, 2, 2, 356, 346, 4, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 6, 58, 54, 2, 431, 2, 7, 32, 2, 16, 11, 94, 2, 10, 10, 4, 2, 2, 7, 4, 2, 2, 2, 2, 8, 2, 8, 2, 121, 31, 7, 27, 86, 2, 2, 16, 6, 465, 2, 2, 2, 2, 17, 2, 42, 4, 2, 37, 473, 6, 2, 6, 2, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 2, 2, 53, 33, 2, 2, 37, 70, 2, 4, 2, 2, 74, 476, 37, 62, 91, 2, 169, 4, 2, 2, 146, 2, 2, 5, 258, 12, 184, 2, 2, 5, 2, 2, 7, 4, 22, 2, 18, 2, 2, 2, 7, 4, 2, 71, 348, 425, 2, 2, 19, 2, 5, 2, 11, 2, 8, 339, 2, 4, 2, 2, 7, 4, 2, 10, 10, 263, 2, 9, 270, 11, 6, 2, 4, 2, 2, 121, 4, 2, 26, 2, 19, 68, 2, 5, 28, 446, 6, 318, 2, 8, 67, 51, 36, 70, 81, 8, 2, 2, 36, 2, 8, 2, 2, 18, 6, 2, 4, 2, 26, 2, 2, 11, 14, 2, 2, 12, 426, 28, 77, 2, 8, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 9, 2, 17, 6, 2, 428, 2, 232, 11, 4, 2, 37, 272, 40, 2, 247, 30, 2, 6, 2, 54, 2, 2, 98, 6, 2, 40, 2, 37, 2, 98, 4, 2, 2, 15, 14, 9, 57, 2, 5, 2, 6, 275, 2, 2, 2, 2, 98, 6, 2, 10, 10, 2, 19, 14, 2, 267, 162, 2, 37, 2, 2, 98, 4, 2, 2, 90, 19, 6, 2, 7, 2, 2, 2, 4, 2, 2, 2, 8, 2, 90, 4, 2, 8, 4, 2, 17, 2, 2, 2, 4, 2, 8, 2, 189, 4, 2, 2, 2, 4, 2, 5, 95, 271, 23, 6, 2, 2, 2, 2, 33, 2, 6, 425, 2, 2, 2, 2, 7, 4, 2, 2, 469, 4, 2, 54, 4, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 5, 2, 68, 2, 19, 2, 2, 4, 2, 7, 263, 65, 2, 34, 6, 2, 2, 43, 159, 29, 9, 2, 9, 387, 73, 195, 2, 10, 10, 2, 4, 58, 2, 54, 14, 2, 117, 22, 16, 93, 5, 2, 4, 192, 15, 12, 16, 93, 34, 6, 2, 2, 33, 4, 2, 7, 15, 2, 2, 2, 325, 12, 62, 30, 2, 8, 67, 14, 17, 6, 2, 44, 148, 2, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 2, 2, 27, 2, 7, 2, 4, 22, 2, 17, 6, 2, 2, 7, 2, 2, 2, 100, 30, 4, 2, 2, 2, 2, 42, 2, 11, 4, 2, 42, 101, 2, 7, 101, 2, 15, 2, 94, 2, 180, 5, 9, 2, 34, 2, 45, 6, 2, 22, 60, 6, 2, 31, 11, 94, 2, 96, 21, 94, 2, 9, 57, 2]),\n",
       "         ...,\n",
       "         list([1, 13, 2, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 4, 2, 2, 2, 2, 2, 395, 2, 5, 2, 11, 119, 2, 89, 2, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2, 284, 2, 2, 37, 315, 4, 226, 20, 272, 2, 40, 29, 152, 60, 181, 8, 30, 50, 2, 362, 80, 119, 12, 21, 2, 2]),\n",
       "         list([1, 11, 119, 241, 9, 4, 2, 20, 12, 468, 15, 94, 2, 2, 2, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2, 7, 2, 46, 2, 9, 2, 5, 4, 2, 47, 8, 79, 90, 145, 164, 162, 50, 6, 2, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 2, 200, 5, 2, 5, 9, 2, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 2, 13, 2, 14, 20, 6, 2, 7, 470]),\n",
       "         list([1, 6, 52, 2, 430, 22, 9, 220, 2, 8, 28, 2, 2, 2, 6, 2, 15, 47, 6, 2, 2, 8, 114, 5, 33, 222, 31, 55, 184, 2, 2, 2, 19, 346, 2, 5, 6, 364, 350, 4, 184, 2, 9, 133, 2, 11, 2, 2, 21, 4, 2, 2, 2, 50, 2, 2, 9, 6, 2, 17, 6, 2, 2, 21, 17, 6, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 2, 19, 4, 78, 173, 7, 27, 2, 2, 2, 2, 2, 9, 6, 2, 17, 210, 5, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 2, 7, 4, 314, 74, 6, 2, 22, 2, 19, 2, 2, 2, 382, 4, 91, 2, 439, 19, 14, 20, 9, 2, 2, 2, 4, 2, 25, 124, 4, 31, 12, 16, 93, 2, 34, 2, 2])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the vocabulary to the top 500 words using num_words\n",
    "imdb.load_data(num_words=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([2, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 2, 173, 36, 256, 2, 25, 100, 43, 838, 112, 50, 670, 2, 2, 35, 480, 284, 2, 150, 2, 172, 112, 167, 2, 336, 385, 39, 2, 172, 2, 2, 17, 546, 38, 13, 447, 2, 192, 50, 16, 2, 147, 2, 19, 14, 22, 2, 2, 2, 469, 2, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 2, 22, 17, 515, 17, 12, 16, 626, 18, 2, 2, 62, 386, 12, 2, 316, 2, 106, 2, 2, 2, 2, 16, 480, 66, 2, 33, 2, 130, 12, 16, 38, 619, 2, 25, 124, 51, 36, 135, 48, 25, 2, 33, 2, 22, 12, 215, 28, 77, 52, 2, 14, 407, 16, 82, 2, 2, 2, 107, 117, 2, 15, 256, 2, 2, 2, 2, 2, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 2, 2, 2, 2, 13, 104, 88, 2, 381, 15, 297, 98, 32, 2, 56, 26, 141, 2, 194, 2, 18, 2, 226, 22, 21, 134, 476, 26, 480, 2, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 2, 226, 65, 16, 38, 2, 88, 12, 16, 283, 2, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "         list([2, 194, 2, 194, 2, 78, 228, 2, 2, 2, 2, 2, 134, 26, 2, 715, 2, 118, 2, 14, 394, 20, 13, 119, 954, 189, 102, 2, 207, 110, 2, 21, 14, 69, 188, 2, 30, 23, 2, 2, 249, 126, 93, 2, 114, 2, 2, 2, 2, 647, 2, 116, 2, 35, 2, 2, 229, 2, 340, 2, 2, 118, 2, 2, 130, 2, 19, 2, 2, 2, 89, 29, 952, 46, 37, 2, 455, 2, 45, 43, 38, 2, 2, 398, 2, 2, 26, 2, 2, 163, 11, 2, 2, 2, 2, 2, 194, 775, 2, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 2, 2, 228, 2, 43, 2, 2, 15, 299, 120, 2, 120, 174, 11, 220, 175, 136, 50, 2, 2, 228, 2, 2, 2, 656, 245, 2, 2, 2, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 2, 2, 371, 78, 22, 625, 64, 2, 2, 2, 168, 145, 23, 2, 2, 15, 16, 2, 2, 2, 28, 2, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([2, 14, 47, 2, 30, 31, 2, 2, 249, 108, 2, 2, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 2, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 2, 86, 320, 35, 534, 19, 263, 2, 2, 2, 2, 33, 89, 78, 12, 66, 16, 2, 360, 2, 2, 58, 316, 334, 11, 2, 2, 43, 645, 662, 2, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 780, 2, 106, 14, 2, 2, 18, 2, 22, 12, 215, 28, 610, 40, 2, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 2, 22, 47, 2, 2, 51, 2, 170, 23, 595, 116, 595, 2, 13, 191, 79, 638, 89, 2, 14, 2, 2, 106, 607, 624, 35, 534, 2, 227, 2, 129, 113]),\n",
       "         ...,\n",
       "         list([2, 11, 2, 230, 245, 2, 2, 2, 2, 446, 2, 45, 2, 84, 2, 2, 21, 2, 912, 84, 2, 325, 725, 134, 2, 2, 84, 2, 36, 28, 57, 2, 21, 2, 140, 2, 703, 2, 2, 84, 56, 18, 2, 14, 2, 31, 2, 2, 2, 2, 2, 2, 2, 18, 2, 20, 207, 110, 563, 12, 2, 2, 2, 2, 97, 2, 20, 53, 2, 74, 2, 460, 364, 2, 29, 270, 11, 960, 108, 45, 40, 29, 2, 395, 11, 2, 2, 500, 2, 2, 89, 364, 70, 29, 140, 2, 64, 2, 11, 2, 2, 26, 178, 2, 529, 443, 2, 2, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 2, 65, 496, 2, 231, 2, 790, 2, 2, 320, 234, 2, 234, 2, 2, 2, 496, 2, 139, 929, 2, 2, 2, 2, 2, 18, 2, 2, 2, 250, 11, 2, 2, 2, 2, 2, 747, 2, 372, 2, 2, 541, 2, 2, 2, 59, 2, 2, 2, 2]),\n",
       "         list([2, 2, 2, 69, 72, 2, 13, 610, 930, 2, 12, 582, 23, 2, 16, 484, 685, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 2, 62, 30, 145, 402, 11, 2, 51, 575, 32, 61, 369, 71, 66, 770, 12, 2, 75, 100, 2, 2, 2, 105, 37, 69, 147, 712, 75, 2, 44, 257, 390, 2, 69, 263, 514, 105, 50, 286, 2, 23, 2, 123, 13, 161, 40, 2, 421, 2, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 2, 719, 2, 13, 18, 31, 62, 40, 2, 2, 2, 2, 2, 14, 123, 2, 942, 25, 2, 721, 12, 145, 2, 202, 12, 160, 580, 202, 12, 2, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 2, 15, 251, 2, 2, 12, 38, 84, 80, 124, 12, 2, 23]),\n",
       "         list([2, 17, 2, 194, 337, 2, 2, 204, 22, 45, 254, 2, 106, 14, 123, 2, 2, 270, 2, 2, 2, 2, 732, 2, 101, 405, 39, 14, 2, 2, 2, 2, 115, 50, 305, 12, 47, 2, 168, 2, 235, 2, 38, 111, 699, 102, 2, 2, 2, 2, 2, 24, 2, 78, 2, 17, 2, 2, 21, 27, 2, 2, 2, 2, 2, 92, 2, 2, 2, 2, 2, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 2, 97, 12, 157, 21, 2, 2, 2, 2, 66, 78, 2, 2, 631, 2, 2, 2, 272, 191, 2, 2, 2, 2, 2, 2, 2, 544, 2, 383, 2, 848, 2, 2, 497, 2, 2, 2, 2, 2, 21, 60, 27, 239, 2, 43, 2, 209, 405, 10, 10, 12, 764, 40, 2, 248, 20, 12, 16, 2, 174, 2, 72, 2, 51, 2, 2, 22, 2, 204, 131, 2])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([2, 591, 202, 14, 31, 2, 717, 10, 10, 2, 2, 2, 2, 360, 2, 2, 177, 2, 394, 354, 2, 123, 2, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 2, 124, 14, 286, 170, 2, 157, 46, 2, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 2, 717]),\n",
       "         list([2, 14, 22, 2, 2, 176, 2, 2, 88, 12, 2, 23, 2, 2, 109, 943, 2, 114, 2, 55, 606, 2, 111, 2, 2, 139, 193, 273, 23, 2, 172, 270, 11, 2, 2, 2, 2, 2, 109, 2, 21, 2, 22, 2, 2, 2, 2, 2, 10, 10, 2, 105, 987, 35, 841, 2, 19, 861, 2, 2, 2, 2, 45, 55, 221, 15, 670, 2, 526, 14, 2, 2, 405, 2, 2, 2, 27, 85, 108, 131, 2, 2, 2, 2, 405, 2, 2, 133, 2, 50, 13, 104, 51, 66, 166, 14, 22, 157, 2, 2, 530, 239, 34, 2, 2, 45, 407, 31, 2, 41, 2, 105, 21, 59, 299, 12, 38, 950, 2, 2, 15, 45, 629, 488, 2, 127, 2, 52, 292, 17, 2, 2, 185, 132, 2, 2, 2, 488, 2, 47, 2, 392, 173, 2, 2, 2, 270, 2, 2, 2, 2, 2, 65, 55, 73, 11, 346, 14, 20, 2, 2, 976, 2, 2, 2, 861, 2, 2, 2, 30, 2, 2, 56, 2, 841, 2, 990, 692, 2, 2, 2, 398, 229, 10, 10, 13, 2, 670, 2, 14, 2, 31, 2, 27, 111, 108, 15, 2, 19, 2, 2, 875, 551, 14, 22, 2, 2, 21, 45, 2, 2, 45, 252, 2, 2, 2, 565, 921, 2, 39, 2, 529, 48, 25, 181, 2, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 2, 290, 2, 58, 10, 10, 472, 45, 55, 878, 2, 169, 11, 374, 2, 25, 203, 28, 2, 818, 12, 125, 2, 2]),\n",
       "         list([2, 111, 748, 2, 2, 2, 2, 2, 87, 2, 2, 2, 31, 318, 2, 2, 2, 498, 2, 748, 63, 29, 2, 220, 686, 2, 2, 17, 12, 575, 220, 2, 17, 2, 185, 132, 2, 16, 53, 928, 11, 2, 74, 2, 438, 21, 27, 2, 589, 2, 22, 107, 2, 2, 997, 2, 2, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 2, 2, 98, 31, 2, 33, 2, 58, 14, 2, 2, 2, 2, 365, 2, 2, 2, 356, 346, 2, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 2, 58, 54, 2, 431, 748, 2, 32, 2, 16, 11, 94, 2, 10, 10, 2, 993, 2, 2, 2, 2, 2, 2, 2, 2, 847, 2, 2, 121, 31, 2, 27, 86, 2, 2, 16, 2, 465, 993, 2, 2, 573, 17, 2, 42, 2, 2, 37, 473, 2, 711, 2, 2, 2, 328, 212, 70, 30, 258, 11, 220, 32, 2, 108, 21, 133, 12, 2, 55, 465, 849, 2, 53, 33, 2, 2, 37, 70, 2, 2, 2, 2, 74, 476, 37, 62, 91, 2, 169, 2, 2, 2, 146, 655, 2, 2, 258, 12, 184, 2, 546, 2, 849, 2, 2, 2, 22, 2, 18, 631, 2, 797, 2, 2, 2, 71, 348, 425, 2, 2, 19, 2, 2, 2, 11, 661, 2, 339, 2, 2, 2, 2, 2, 2, 2, 10, 10, 263, 787, 2, 270, 11, 2, 2, 2, 2, 2, 121, 2, 2, 26, 2, 19, 68, 2, 2, 28, 446, 2, 318, 2, 2, 67, 51, 36, 70, 81, 2, 2, 2, 36, 2, 2, 2, 2, 18, 2, 711, 2, 2, 26, 2, 2, 11, 14, 636, 720, 12, 426, 28, 77, 776, 2, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 2, 2, 17, 2, 2, 428, 2, 232, 11, 2, 2, 37, 272, 40, 2, 247, 30, 656, 2, 2, 54, 2, 2, 98, 2, 2, 40, 558, 37, 2, 98, 2, 2, 2, 15, 14, 2, 57, 2, 2, 2, 2, 275, 711, 2, 2, 2, 98, 2, 2, 10, 10, 2, 19, 14, 2, 267, 162, 711, 37, 2, 752, 98, 2, 2, 2, 90, 19, 2, 2, 2, 2, 2, 2, 2, 2, 2, 930, 2, 508, 90, 2, 2, 2, 2, 2, 17, 2, 2, 2, 2, 2, 2, 2, 189, 2, 2, 2, 2, 2, 2, 2, 95, 271, 23, 2, 2, 2, 2, 2, 33, 2, 2, 425, 2, 2, 2, 2, 2, 2, 2, 2, 469, 2, 2, 54, 2, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 2, 2, 68, 2, 19, 2, 2, 2, 2, 2, 263, 65, 2, 34, 2, 2, 2, 43, 159, 29, 2, 2, 2, 387, 73, 195, 584, 10, 10, 2, 2, 58, 810, 54, 14, 2, 117, 22, 16, 93, 2, 2, 2, 192, 15, 12, 16, 93, 34, 2, 2, 2, 33, 2, 2, 2, 15, 2, 2, 2, 325, 12, 62, 30, 776, 2, 67, 14, 17, 2, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 2, 2, 819, 2, 22, 2, 17, 2, 2, 787, 2, 2, 2, 2, 100, 30, 2, 2, 2, 2, 2, 42, 2, 11, 2, 2, 42, 101, 704, 2, 101, 999, 15, 2, 94, 2, 180, 2, 2, 2, 34, 2, 45, 2, 2, 22, 60, 2, 2, 31, 11, 94, 2, 96, 21, 94, 749, 2, 57, 975]),\n",
       "         ...,\n",
       "         list([2, 13, 2, 15, 2, 135, 14, 2, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 2, 2, 910, 769, 2, 2, 395, 2, 2, 2, 11, 119, 2, 89, 2, 2, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 2, 185, 2, 284, 2, 2, 37, 315, 2, 226, 20, 272, 2, 40, 29, 152, 60, 181, 2, 30, 50, 553, 362, 80, 119, 12, 21, 846, 2]),\n",
       "         list([2, 11, 119, 241, 2, 2, 840, 20, 12, 468, 15, 94, 2, 562, 791, 39, 2, 86, 107, 2, 97, 14, 31, 33, 2, 2, 2, 743, 46, 2, 2, 2, 2, 2, 768, 47, 2, 79, 90, 145, 164, 162, 50, 2, 501, 119, 2, 2, 2, 78, 232, 15, 16, 224, 11, 2, 333, 20, 2, 985, 200, 2, 2, 2, 2, 2, 2, 79, 357, 2, 20, 47, 220, 57, 206, 139, 11, 12, 2, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 536, 13, 520, 14, 20, 2, 2, 2, 470]),\n",
       "         list([2, 2, 52, 2, 430, 22, 2, 220, 2, 2, 28, 2, 519, 2, 2, 769, 15, 47, 2, 2, 2, 2, 114, 2, 33, 222, 31, 55, 184, 704, 2, 2, 19, 346, 2, 2, 2, 364, 350, 2, 184, 2, 2, 133, 2, 11, 2, 2, 21, 2, 2, 2, 570, 50, 2, 2, 2, 2, 2, 17, 2, 2, 2, 21, 17, 2, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 2, 194, 21, 29, 218, 2, 19, 2, 78, 173, 2, 27, 2, 2, 2, 718, 2, 2, 2, 2, 17, 210, 2, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 2, 392, 217, 21, 50, 2, 57, 65, 12, 2, 53, 40, 35, 390, 2, 11, 2, 2, 2, 2, 314, 74, 2, 792, 22, 2, 19, 714, 727, 2, 382, 2, 91, 2, 439, 19, 14, 20, 2, 2, 2, 2, 2, 756, 25, 124, 2, 31, 12, 16, 93, 804, 34, 2, 2])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore the top 10 most frequent words\n",
    "imdb.load_data(skip_top=10, num_words=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
       "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "         list([1, 13, 1228, 119, 14, 552, 7, 20, 190, 14, 58, 13, 258, 546, 1786, 8, 1968, 4, 268, 237, 13, 191, 81, 15, 13, 80, 43, 3824, 44, 12, 14, 16, 427, 3192, 4, 183, 15, 593, 19, 4, 351, 362, 26, 55, 646, 21, 4, 1239, 84, 26, 1557, 3755, 13, 244, 6, 2071, 132, 184, 194, 5, 13, 70, 4478, 546, 73, 190, 13, 62, 24, 81, 320, 4, 538, 4, 117, 250, 127, 11, 14, 20, 82, 4, 452, 11, 14, 20, 9, 8654, 19, 41, 476, 8, 4, 213, 7, 9185, 13, 657, 13, 286, 38, 1612, 44, 41, 5, 41, 1729, 88, 13, 62, 28, 900, 510, 4, 509, 51, 6, 612, 59, 16, 193, 61, 4666, 5, 702, 930, 143, 285, 25, 67, 41, 81, 366, 4, 130, 82, 9, 259, 334, 397, 1195, 7, 149, 102, 15, 26, 814, 38, 465, 1627, 31, 70, 983, 67, 51, 9, 112, 814, 17, 35, 311, 75, 26, 11649, 574, 19, 4, 1729, 23, 4, 268, 38, 95, 138, 4, 609, 191, 75, 28, 314, 1772]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 0, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.load_data(maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
       "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "        dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0], dtype=int64)),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "         list([1, 111, 748, 4368, 1133, 33782, 24563, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 24563, 16, 53, 928, 11, 51278, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 20123, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 48078, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 27608, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 45222, 7, 4, 1766, 2634, 2164, 24563, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 30995, 573, 17, 61862, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 61862, 48414, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 68411, 25399, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 25399, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 25399, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 25399, 3292, 98, 6, 31036, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 73284, 7, 36744, 1810, 77553, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 48414, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 31036, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 21627, 5437, 33, 1526, 6, 425, 3155, 33697, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 68411, 25399, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 28228, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 24563, 203, 42, 203, 24, 28, 69, 32157, 6676, 11, 330, 54, 29, 93, 61862, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 61862, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0], dtype=int64)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use '1' as the character that indicates the start of a sequence\n",
    "imdb.load_data(start_char=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the word index as a dictionary\n",
    "# Accounting for index_from\n",
    "index_from = 3\n",
    "test = {key: value + index_from for key, value in imdb_word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34704,\n",
       " 'tsukino': 52009,\n",
       " 'nunnery': 52010,\n",
       " 'sonja': 16819,\n",
       " 'vani': 63954,\n",
       " 'woods': 1411,\n",
       " 'spiders': 16118,\n",
       " 'hanging': 2348,\n",
       " 'woody': 2292,\n",
       " 'trawling': 52011,\n",
       " \"hold's\": 52012,\n",
       " 'comically': 11310,\n",
       " 'localized': 40833,\n",
       " 'disobeying': 30571,\n",
       " \"'royale\": 52013,\n",
       " \"harpo's\": 40834,\n",
       " 'canet': 52014,\n",
       " 'aileen': 19316,\n",
       " 'acurately': 52015,\n",
       " \"diplomat's\": 52016,\n",
       " 'rickman': 25245,\n",
       " 'arranged': 6749,\n",
       " 'rumbustious': 52017,\n",
       " 'familiarness': 52018,\n",
       " \"spider'\": 52019,\n",
       " 'hahahah': 68807,\n",
       " \"wood'\": 52020,\n",
       " 'transvestism': 40836,\n",
       " \"hangin'\": 34705,\n",
       " 'bringing': 2341,\n",
       " 'seamier': 40837,\n",
       " 'wooded': 34706,\n",
       " 'bravora': 52021,\n",
       " 'grueling': 16820,\n",
       " 'wooden': 1639,\n",
       " 'wednesday': 16821,\n",
       " \"'prix\": 52022,\n",
       " 'altagracia': 34707,\n",
       " 'circuitry': 52023,\n",
       " 'crotch': 11588,\n",
       " 'busybody': 57769,\n",
       " \"tart'n'tangy\": 52024,\n",
       " 'burgade': 14132,\n",
       " 'thrace': 52026,\n",
       " \"tom's\": 11041,\n",
       " 'snuggles': 52028,\n",
       " 'francesco': 29117,\n",
       " 'complainers': 52030,\n",
       " 'templarios': 52128,\n",
       " '272': 40838,\n",
       " '273': 52031,\n",
       " 'zaniacs': 52133,\n",
       " '275': 34709,\n",
       " 'consenting': 27634,\n",
       " 'snuggled': 40839,\n",
       " 'inanimate': 15495,\n",
       " 'uality': 52033,\n",
       " 'bronte': 11929,\n",
       " 'errors': 4013,\n",
       " 'dialogs': 3233,\n",
       " \"yomada's\": 52034,\n",
       " \"madman's\": 34710,\n",
       " 'dialoge': 30588,\n",
       " 'usenet': 52036,\n",
       " 'videodrome': 40840,\n",
       " \"kid'\": 26341,\n",
       " 'pawed': 52037,\n",
       " \"'girlfriend'\": 30572,\n",
       " \"'pleasure\": 52038,\n",
       " \"'reloaded'\": 52039,\n",
       " \"kazakos'\": 40842,\n",
       " 'rocque': 52040,\n",
       " 'mailings': 52041,\n",
       " 'brainwashed': 11930,\n",
       " 'mcanally': 16822,\n",
       " \"tom''\": 52042,\n",
       " 'kurupt': 25246,\n",
       " 'affiliated': 21908,\n",
       " 'babaganoosh': 52043,\n",
       " \"noe's\": 40843,\n",
       " 'quart': 40844,\n",
       " 'kids': 362,\n",
       " 'uplifting': 5037,\n",
       " 'controversy': 7096,\n",
       " 'kida': 21909,\n",
       " 'kidd': 23382,\n",
       " \"error'\": 52044,\n",
       " 'neurologist': 52045,\n",
       " 'spotty': 18513,\n",
       " 'cobblers': 30573,\n",
       " 'projection': 9881,\n",
       " 'fastforwarding': 40845,\n",
       " 'sters': 52046,\n",
       " \"eggar's\": 52047,\n",
       " 'etherything': 52048,\n",
       " 'gateshead': 40846,\n",
       " 'airball': 34711,\n",
       " 'unsinkable': 25247,\n",
       " 'stern': 7183,\n",
       " \"cervi's\": 52049,\n",
       " 'dnd': 40847,\n",
       " 'dna': 11589,\n",
       " 'insecurity': 20601,\n",
       " \"'reboot'\": 52050,\n",
       " 'trelkovsky': 11040,\n",
       " 'jaekel': 52051,\n",
       " 'sidebars': 52052,\n",
       " \"sforza's\": 52053,\n",
       " 'distortions': 17636,\n",
       " 'mutinies': 52054,\n",
       " 'sermons': 30605,\n",
       " '7ft': 40849,\n",
       " 'boobage': 52055,\n",
       " \"o'bannon's\": 52056,\n",
       " 'populations': 23383,\n",
       " 'chulak': 52057,\n",
       " 'mesmerize': 27636,\n",
       " 'quinnell': 52058,\n",
       " 'yahoo': 10310,\n",
       " 'meteorologist': 52060,\n",
       " 'beswick': 42580,\n",
       " 'boorman': 15496,\n",
       " 'voicework': 40850,\n",
       " \"ster'\": 52061,\n",
       " 'blustering': 22925,\n",
       " 'hj': 52062,\n",
       " 'intake': 27637,\n",
       " 'morally': 5624,\n",
       " 'jumbling': 40852,\n",
       " 'bowersock': 52063,\n",
       " \"'porky's'\": 52064,\n",
       " 'gershon': 16824,\n",
       " 'ludicrosity': 40853,\n",
       " 'coprophilia': 52065,\n",
       " 'expressively': 40854,\n",
       " \"india's\": 19503,\n",
       " \"post's\": 34713,\n",
       " 'wana': 52066,\n",
       " 'wang': 5286,\n",
       " 'wand': 30574,\n",
       " 'wane': 25248,\n",
       " 'edgeways': 52324,\n",
       " 'titanium': 34714,\n",
       " 'pinta': 40855,\n",
       " 'want': 181,\n",
       " 'pinto': 30575,\n",
       " 'whoopdedoodles': 52068,\n",
       " 'tchaikovsky': 21911,\n",
       " 'travel': 2106,\n",
       " \"'victory'\": 52069,\n",
       " 'copious': 11931,\n",
       " 'gouge': 22436,\n",
       " \"chapters'\": 52070,\n",
       " 'barbra': 6705,\n",
       " 'uselessness': 30576,\n",
       " \"wan'\": 52071,\n",
       " 'assimilated': 27638,\n",
       " 'petiot': 16119,\n",
       " 'most\\x85and': 52072,\n",
       " 'dinosaurs': 3933,\n",
       " 'wrong': 355,\n",
       " 'seda': 52073,\n",
       " 'stollen': 52074,\n",
       " 'sentencing': 34715,\n",
       " 'ouroboros': 40856,\n",
       " 'assimilates': 40857,\n",
       " 'colorfully': 40858,\n",
       " 'glenne': 27639,\n",
       " 'dongen': 52075,\n",
       " 'subplots': 4763,\n",
       " 'kiloton': 52076,\n",
       " 'chandon': 23384,\n",
       " \"effect'\": 34716,\n",
       " 'snugly': 27640,\n",
       " 'kuei': 40859,\n",
       " 'welcomed': 9095,\n",
       " 'dishonor': 30074,\n",
       " 'concurrence': 52078,\n",
       " 'stoicism': 23385,\n",
       " \"guys'\": 14899,\n",
       " \"beroemd'\": 52080,\n",
       " 'butcher': 6706,\n",
       " \"melfi's\": 40860,\n",
       " 'aargh': 30626,\n",
       " 'playhouse': 20602,\n",
       " 'wickedly': 11311,\n",
       " 'fit': 1183,\n",
       " 'labratory': 52081,\n",
       " 'lifeline': 40862,\n",
       " 'screaming': 1930,\n",
       " 'fix': 4290,\n",
       " 'cineliterate': 52082,\n",
       " 'fic': 52083,\n",
       " 'fia': 52084,\n",
       " 'fig': 34717,\n",
       " 'fmvs': 52085,\n",
       " 'fie': 52086,\n",
       " 'reentered': 52087,\n",
       " 'fin': 30577,\n",
       " 'doctresses': 52088,\n",
       " 'fil': 52089,\n",
       " 'zucker': 12609,\n",
       " 'ached': 31934,\n",
       " 'counsil': 52091,\n",
       " 'paterfamilias': 52092,\n",
       " 'songwriter': 13888,\n",
       " 'shivam': 34718,\n",
       " 'hurting': 9657,\n",
       " 'effects': 302,\n",
       " 'slauther': 52093,\n",
       " \"'flame'\": 52094,\n",
       " 'sommerset': 52095,\n",
       " 'interwhined': 52096,\n",
       " 'whacking': 27641,\n",
       " 'bartok': 52097,\n",
       " 'barton': 8778,\n",
       " 'frewer': 21912,\n",
       " \"fi'\": 52098,\n",
       " 'ingrid': 6195,\n",
       " 'stribor': 30578,\n",
       " 'approporiately': 52099,\n",
       " 'wobblyhand': 52100,\n",
       " 'tantalisingly': 52101,\n",
       " 'ankylosaurus': 52102,\n",
       " 'parasites': 17637,\n",
       " 'childen': 52103,\n",
       " \"jenkins'\": 52104,\n",
       " 'metafiction': 52105,\n",
       " 'golem': 17638,\n",
       " 'indiscretion': 40863,\n",
       " \"reeves'\": 23386,\n",
       " \"inamorata's\": 57784,\n",
       " 'brittannica': 52107,\n",
       " 'adapt': 7919,\n",
       " \"russo's\": 30579,\n",
       " 'guitarists': 48249,\n",
       " 'abbott': 10556,\n",
       " 'abbots': 40864,\n",
       " 'lanisha': 17652,\n",
       " 'magickal': 40866,\n",
       " 'mattter': 52108,\n",
       " \"'willy\": 52109,\n",
       " 'pumpkins': 34719,\n",
       " 'stuntpeople': 52110,\n",
       " 'estimate': 30580,\n",
       " 'ugghhh': 40867,\n",
       " 'gameplay': 11312,\n",
       " \"wern't\": 52111,\n",
       " \"n'sync\": 40868,\n",
       " 'sickeningly': 16120,\n",
       " 'chiara': 40869,\n",
       " 'disturbed': 4014,\n",
       " 'portmanteau': 40870,\n",
       " 'ineffectively': 52112,\n",
       " \"duchonvey's\": 82146,\n",
       " \"nasty'\": 37522,\n",
       " 'purpose': 1288,\n",
       " 'lazers': 52115,\n",
       " 'lightened': 28108,\n",
       " 'kaliganj': 52116,\n",
       " 'popularism': 52117,\n",
       " \"damme's\": 18514,\n",
       " 'stylistics': 30581,\n",
       " 'mindgaming': 52118,\n",
       " 'spoilerish': 46452,\n",
       " \"'corny'\": 52120,\n",
       " 'boerner': 34721,\n",
       " 'olds': 6795,\n",
       " 'bakelite': 52121,\n",
       " 'renovated': 27642,\n",
       " 'forrester': 27643,\n",
       " \"lumiere's\": 52122,\n",
       " 'gaskets': 52027,\n",
       " 'needed': 887,\n",
       " 'smight': 34722,\n",
       " 'master': 1300,\n",
       " \"edie's\": 25908,\n",
       " 'seeber': 40871,\n",
       " 'hiya': 52123,\n",
       " 'fuzziness': 52124,\n",
       " 'genesis': 14900,\n",
       " 'rewards': 12610,\n",
       " 'enthrall': 30582,\n",
       " \"'about\": 40872,\n",
       " \"recollection's\": 52125,\n",
       " 'mutilated': 11042,\n",
       " 'fatherlands': 52126,\n",
       " \"fischer's\": 52127,\n",
       " 'positively': 5402,\n",
       " '270': 34708,\n",
       " 'ahmed': 34723,\n",
       " 'zatoichi': 9839,\n",
       " 'bannister': 13889,\n",
       " 'anniversaries': 52130,\n",
       " \"helm's\": 30583,\n",
       " \"'work'\": 52131,\n",
       " 'exclaimed': 34724,\n",
       " \"'unfunny'\": 52132,\n",
       " '274': 52032,\n",
       " 'feeling': 547,\n",
       " \"wanda's\": 52134,\n",
       " 'dolan': 33269,\n",
       " '278': 52136,\n",
       " 'peacoat': 52137,\n",
       " 'brawny': 40873,\n",
       " 'mishra': 40874,\n",
       " 'worlders': 40875,\n",
       " 'protags': 52138,\n",
       " 'skullcap': 52139,\n",
       " 'dastagir': 57599,\n",
       " 'affairs': 5625,\n",
       " 'wholesome': 7802,\n",
       " 'hymen': 52140,\n",
       " 'paramedics': 25249,\n",
       " 'unpersons': 52141,\n",
       " 'heavyarms': 52142,\n",
       " 'affaire': 52143,\n",
       " 'coulisses': 52144,\n",
       " 'hymer': 40876,\n",
       " 'kremlin': 52145,\n",
       " 'shipments': 30584,\n",
       " 'pixilated': 52146,\n",
       " \"'00s\": 30585,\n",
       " 'diminishing': 18515,\n",
       " 'cinematic': 1360,\n",
       " 'resonates': 14901,\n",
       " 'simplify': 40877,\n",
       " \"nature'\": 40878,\n",
       " 'temptresses': 40879,\n",
       " 'reverence': 16825,\n",
       " 'resonated': 19505,\n",
       " 'dailey': 34725,\n",
       " '2\\x85': 52147,\n",
       " 'treize': 27644,\n",
       " 'majo': 52148,\n",
       " 'kiya': 21913,\n",
       " 'woolnough': 52149,\n",
       " 'thanatos': 39800,\n",
       " 'sandoval': 35734,\n",
       " 'dorama': 40882,\n",
       " \"o'shaughnessy\": 52150,\n",
       " 'tech': 4991,\n",
       " 'fugitives': 32021,\n",
       " 'teck': 30586,\n",
       " \"'e'\": 76128,\n",
       " 'doesnt': 40884,\n",
       " 'purged': 52152,\n",
       " 'saying': 660,\n",
       " \"martians'\": 41098,\n",
       " 'norliss': 23421,\n",
       " 'dickey': 27645,\n",
       " 'dicker': 52155,\n",
       " \"'sependipity\": 52156,\n",
       " 'padded': 8425,\n",
       " 'ordell': 57795,\n",
       " \"sturges'\": 40885,\n",
       " 'independentcritics': 52157,\n",
       " 'tempted': 5748,\n",
       " \"atkinson's\": 34727,\n",
       " 'hounded': 25250,\n",
       " 'apace': 52158,\n",
       " 'clicked': 15497,\n",
       " \"'humor'\": 30587,\n",
       " \"martino's\": 17180,\n",
       " \"'supporting\": 52159,\n",
       " 'warmongering': 52035,\n",
       " \"zemeckis's\": 34728,\n",
       " 'lube': 21914,\n",
       " 'shocky': 52160,\n",
       " 'plate': 7479,\n",
       " 'plata': 40886,\n",
       " 'sturgess': 40887,\n",
       " \"nerds'\": 40888,\n",
       " 'plato': 20603,\n",
       " 'plath': 34729,\n",
       " 'platt': 40889,\n",
       " 'mcnab': 52162,\n",
       " 'clumsiness': 27646,\n",
       " 'altogether': 3902,\n",
       " 'massacring': 42587,\n",
       " 'bicenntinial': 52163,\n",
       " 'skaal': 40890,\n",
       " 'droning': 14363,\n",
       " 'lds': 8779,\n",
       " 'jaguar': 21915,\n",
       " \"cale's\": 34730,\n",
       " 'nicely': 1780,\n",
       " 'mummy': 4591,\n",
       " \"lot's\": 18516,\n",
       " 'patch': 10089,\n",
       " 'kerkhof': 50205,\n",
       " \"leader's\": 52164,\n",
       " \"'movie\": 27647,\n",
       " 'uncomfirmed': 52165,\n",
       " 'heirloom': 40891,\n",
       " 'wrangle': 47363,\n",
       " 'emotion\\x85': 52166,\n",
       " \"'stargate'\": 52167,\n",
       " 'pinoy': 40892,\n",
       " 'conchatta': 40893,\n",
       " 'broeke': 41131,\n",
       " 'advisedly': 40894,\n",
       " \"barker's\": 17639,\n",
       " 'descours': 52169,\n",
       " 'lots': 775,\n",
       " 'lotr': 9262,\n",
       " 'irs': 9882,\n",
       " 'lott': 52170,\n",
       " 'xvi': 40895,\n",
       " 'irk': 34731,\n",
       " 'irl': 52171,\n",
       " 'ira': 6890,\n",
       " 'belzer': 21916,\n",
       " 'irc': 52172,\n",
       " 'ire': 27648,\n",
       " 'requisites': 40896,\n",
       " 'discipline': 7696,\n",
       " 'lyoko': 52964,\n",
       " 'extend': 11313,\n",
       " 'nature': 876,\n",
       " \"'dickie'\": 52173,\n",
       " 'optimist': 40897,\n",
       " 'lapping': 30589,\n",
       " 'superficial': 3903,\n",
       " 'vestment': 52174,\n",
       " 'extent': 2826,\n",
       " 'tendons': 52175,\n",
       " \"heller's\": 52176,\n",
       " 'quagmires': 52177,\n",
       " 'miyako': 52178,\n",
       " 'moocow': 20604,\n",
       " \"coles'\": 52179,\n",
       " 'lookit': 40898,\n",
       " 'ravenously': 52180,\n",
       " 'levitating': 40899,\n",
       " 'perfunctorily': 52181,\n",
       " 'lookin': 30590,\n",
       " \"lot'\": 40901,\n",
       " 'lookie': 52182,\n",
       " 'fearlessly': 34873,\n",
       " 'libyan': 52184,\n",
       " 'fondles': 40902,\n",
       " 'gopher': 35717,\n",
       " 'wearying': 40904,\n",
       " \"nz's\": 52185,\n",
       " 'minuses': 27649,\n",
       " 'puposelessly': 52186,\n",
       " 'shandling': 52187,\n",
       " 'decapitates': 31271,\n",
       " 'humming': 11932,\n",
       " \"'nother\": 40905,\n",
       " 'smackdown': 21917,\n",
       " 'underdone': 30591,\n",
       " 'frf': 40906,\n",
       " 'triviality': 52188,\n",
       " 'fro': 25251,\n",
       " 'bothers': 8780,\n",
       " \"'kensington\": 52189,\n",
       " 'much': 76,\n",
       " 'muco': 34733,\n",
       " 'wiseguy': 22618,\n",
       " \"richie's\": 27651,\n",
       " 'tonino': 40907,\n",
       " 'unleavened': 52190,\n",
       " 'fry': 11590,\n",
       " \"'tv'\": 40908,\n",
       " 'toning': 40909,\n",
       " 'obese': 14364,\n",
       " 'sensationalized': 30592,\n",
       " 'spiv': 40910,\n",
       " 'spit': 6262,\n",
       " 'arkin': 7367,\n",
       " 'charleton': 21918,\n",
       " 'jeon': 16826,\n",
       " 'boardroom': 21919,\n",
       " 'doubts': 4992,\n",
       " 'spin': 3087,\n",
       " 'hepo': 53086,\n",
       " 'wildcat': 27652,\n",
       " 'venoms': 10587,\n",
       " 'misconstrues': 52194,\n",
       " 'mesmerising': 18517,\n",
       " 'misconstrued': 40911,\n",
       " 'rescinds': 52195,\n",
       " 'prostrate': 52196,\n",
       " 'majid': 40912,\n",
       " 'climbed': 16482,\n",
       " 'canoeing': 34734,\n",
       " 'majin': 52198,\n",
       " 'animie': 57807,\n",
       " 'sylke': 40913,\n",
       " 'conditioned': 14902,\n",
       " 'waddell': 40914,\n",
       " '3\\x85': 52199,\n",
       " 'hyperdrive': 41191,\n",
       " 'conditioner': 34735,\n",
       " 'bricklayer': 53156,\n",
       " 'hong': 2579,\n",
       " 'memoriam': 52201,\n",
       " 'inventively': 30595,\n",
       " \"levant's\": 25252,\n",
       " 'portobello': 20641,\n",
       " 'remand': 52203,\n",
       " 'mummified': 19507,\n",
       " 'honk': 27653,\n",
       " 'spews': 19508,\n",
       " 'visitations': 40915,\n",
       " 'mummifies': 52204,\n",
       " 'cavanaugh': 25253,\n",
       " 'zeon': 23388,\n",
       " \"jungle's\": 40916,\n",
       " 'viertel': 34736,\n",
       " 'frenchmen': 27654,\n",
       " 'torpedoes': 52205,\n",
       " 'schlessinger': 52206,\n",
       " 'torpedoed': 34737,\n",
       " 'blister': 69879,\n",
       " 'cinefest': 52207,\n",
       " 'furlough': 34738,\n",
       " 'mainsequence': 52208,\n",
       " 'mentors': 40917,\n",
       " 'academic': 9097,\n",
       " 'stillness': 20605,\n",
       " 'academia': 40918,\n",
       " 'lonelier': 52209,\n",
       " 'nibby': 52210,\n",
       " \"losers'\": 52211,\n",
       " 'cineastes': 40919,\n",
       " 'corporate': 4452,\n",
       " 'massaging': 40920,\n",
       " 'bellow': 30596,\n",
       " 'absurdities': 19509,\n",
       " 'expetations': 53244,\n",
       " 'nyfiken': 40921,\n",
       " 'mehras': 75641,\n",
       " 'lasse': 52212,\n",
       " 'visability': 52213,\n",
       " 'militarily': 33949,\n",
       " \"elder'\": 52214,\n",
       " 'gainsbourg': 19026,\n",
       " 'hah': 20606,\n",
       " 'hai': 13423,\n",
       " 'haj': 34739,\n",
       " 'hak': 25254,\n",
       " 'hal': 4314,\n",
       " 'ham': 4895,\n",
       " 'duffer': 53262,\n",
       " 'haa': 52216,\n",
       " 'had': 69,\n",
       " 'advancement': 11933,\n",
       " 'hag': 16828,\n",
       " \"hand'\": 25255,\n",
       " 'hay': 13424,\n",
       " 'mcnamara': 20607,\n",
       " \"mozart's\": 52217,\n",
       " 'duffel': 30734,\n",
       " 'haq': 30597,\n",
       " 'har': 13890,\n",
       " 'has': 47,\n",
       " 'hat': 2404,\n",
       " 'hav': 40922,\n",
       " 'haw': 30598,\n",
       " 'figtings': 52218,\n",
       " 'elders': 15498,\n",
       " 'underpanted': 52219,\n",
       " 'pninson': 52220,\n",
       " 'unequivocally': 27655,\n",
       " \"barbara's\": 23676,\n",
       " \"bello'\": 52222,\n",
       " 'indicative': 13000,\n",
       " 'yawnfest': 40923,\n",
       " 'hexploitation': 52223,\n",
       " \"loder's\": 52224,\n",
       " 'sleuthing': 27656,\n",
       " \"justin's\": 32625,\n",
       " \"'ball\": 52225,\n",
       " \"'summer\": 52226,\n",
       " \"'demons'\": 34938,\n",
       " \"mormon's\": 52228,\n",
       " \"laughton's\": 34740,\n",
       " 'debell': 52229,\n",
       " 'shipyard': 39727,\n",
       " 'unabashedly': 30600,\n",
       " 'disks': 40404,\n",
       " 'crowd': 2293,\n",
       " 'crowe': 10090,\n",
       " \"vancouver's\": 56437,\n",
       " 'mosques': 34741,\n",
       " 'crown': 6630,\n",
       " 'culpas': 52230,\n",
       " 'crows': 27657,\n",
       " 'surrell': 53347,\n",
       " 'flowless': 52232,\n",
       " 'sheirk': 52233,\n",
       " \"'three\": 40926,\n",
       " \"peterson'\": 52234,\n",
       " 'ooverall': 52235,\n",
       " 'perchance': 40927,\n",
       " 'bottom': 1324,\n",
       " 'chabert': 53366,\n",
       " 'sneha': 52236,\n",
       " 'inhuman': 13891,\n",
       " 'ichii': 52237,\n",
       " 'ursla': 52238,\n",
       " 'completly': 30601,\n",
       " 'moviedom': 40928,\n",
       " 'raddick': 52239,\n",
       " 'brundage': 51998,\n",
       " 'brigades': 40929,\n",
       " 'starring': 1184,\n",
       " \"'goal'\": 52240,\n",
       " 'caskets': 52241,\n",
       " 'willcock': 52242,\n",
       " \"threesome's\": 52243,\n",
       " \"mosque'\": 52244,\n",
       " \"cover's\": 52245,\n",
       " 'spaceships': 17640,\n",
       " 'anomalous': 40930,\n",
       " 'ptsd': 27658,\n",
       " 'shirdan': 52246,\n",
       " 'obscenity': 21965,\n",
       " 'lemmings': 30602,\n",
       " 'duccio': 30603,\n",
       " \"levene's\": 52247,\n",
       " \"'gorby'\": 52248,\n",
       " \"teenager's\": 25258,\n",
       " 'marshall': 5343,\n",
       " 'honeymoon': 9098,\n",
       " 'shoots': 3234,\n",
       " 'despised': 12261,\n",
       " 'okabasho': 52249,\n",
       " 'fabric': 8292,\n",
       " 'cannavale': 18518,\n",
       " 'raped': 3540,\n",
       " \"tutt's\": 52250,\n",
       " 'grasping': 17641,\n",
       " 'despises': 18519,\n",
       " \"thief's\": 40931,\n",
       " 'rapes': 8929,\n",
       " 'raper': 52251,\n",
       " \"eyre'\": 27659,\n",
       " 'walchek': 52252,\n",
       " \"elmo's\": 23389,\n",
       " 'perfumes': 40932,\n",
       " 'spurting': 21921,\n",
       " \"exposition'\\x85\": 52253,\n",
       " 'denoting': 52254,\n",
       " 'thesaurus': 34743,\n",
       " \"shoot'\": 40933,\n",
       " 'bonejack': 49762,\n",
       " 'simpsonian': 52256,\n",
       " 'hebetude': 30604,\n",
       " \"hallow's\": 34744,\n",
       " 'desperation\\x85': 52257,\n",
       " 'incinerator': 34745,\n",
       " 'congratulations': 10311,\n",
       " 'humbled': 52258,\n",
       " \"else's\": 5927,\n",
       " 'trelkovski': 40848,\n",
       " \"rape'\": 52259,\n",
       " \"'chapters'\": 59389,\n",
       " '1600s': 52260,\n",
       " 'martian': 7256,\n",
       " 'nicest': 25259,\n",
       " 'eyred': 52262,\n",
       " 'passenger': 9460,\n",
       " 'disgrace': 6044,\n",
       " 'moderne': 52263,\n",
       " 'barrymore': 5123,\n",
       " 'yankovich': 52264,\n",
       " 'moderns': 40934,\n",
       " 'studliest': 52265,\n",
       " 'bedsheet': 52266,\n",
       " 'decapitation': 14903,\n",
       " 'slurring': 52267,\n",
       " \"'nunsploitation'\": 52268,\n",
       " \"'character'\": 34746,\n",
       " 'cambodia': 9883,\n",
       " 'rebelious': 52269,\n",
       " 'pasadena': 27660,\n",
       " 'crowne': 40935,\n",
       " \"'bedchamber\": 52270,\n",
       " 'conjectural': 52271,\n",
       " 'appologize': 52272,\n",
       " 'halfassing': 52273,\n",
       " 'paycheque': 57819,\n",
       " 'palms': 20609,\n",
       " \"'islands\": 52274,\n",
       " 'hawked': 40936,\n",
       " 'palme': 21922,\n",
       " 'conservatively': 40937,\n",
       " 'larp': 64010,\n",
       " 'palma': 5561,\n",
       " 'smelling': 21923,\n",
       " 'aragorn': 13001,\n",
       " 'hawker': 52275,\n",
       " 'hawkes': 52276,\n",
       " 'explosions': 3978,\n",
       " 'loren': 8062,\n",
       " \"pyle's\": 52277,\n",
       " 'shootout': 6707,\n",
       " \"mike's\": 18520,\n",
       " \"driscoll's\": 52278,\n",
       " 'cogsworth': 40938,\n",
       " \"britian's\": 52279,\n",
       " 'childs': 34747,\n",
       " \"portrait's\": 52280,\n",
       " 'chain': 3629,\n",
       " 'whoever': 2500,\n",
       " 'puttered': 52281,\n",
       " 'childe': 52282,\n",
       " 'maywether': 52283,\n",
       " 'chair': 3039,\n",
       " \"rance's\": 52284,\n",
       " 'machu': 34748,\n",
       " 'ballet': 4520,\n",
       " 'grapples': 34749,\n",
       " 'summerize': 76155,\n",
       " 'freelance': 30606,\n",
       " \"andrea's\": 52286,\n",
       " '\\x91very': 52287,\n",
       " 'coolidge': 45882,\n",
       " 'mache': 18521,\n",
       " 'balled': 52288,\n",
       " 'grappled': 40940,\n",
       " 'macha': 18522,\n",
       " 'underlining': 21924,\n",
       " 'macho': 5626,\n",
       " 'oversight': 19510,\n",
       " 'machi': 25260,\n",
       " 'verbally': 11314,\n",
       " 'tenacious': 21925,\n",
       " 'windshields': 40941,\n",
       " 'paychecks': 18560,\n",
       " 'jerk': 3399,\n",
       " \"good'\": 11934,\n",
       " 'prancer': 34751,\n",
       " 'prances': 21926,\n",
       " 'olympus': 52289,\n",
       " 'lark': 21927,\n",
       " 'embark': 10788,\n",
       " 'gloomy': 7368,\n",
       " 'jehaan': 52290,\n",
       " 'turaqui': 52291,\n",
       " \"child'\": 20610,\n",
       " 'locked': 2897,\n",
       " 'pranced': 52292,\n",
       " 'exact': 2591,\n",
       " 'unattuned': 52293,\n",
       " 'minute': 786,\n",
       " 'skewed': 16121,\n",
       " 'hodgins': 40943,\n",
       " 'skewer': 34752,\n",
       " 'think\\x85': 52294,\n",
       " 'rosenstein': 38768,\n",
       " 'helmit': 52295,\n",
       " 'wrestlemanias': 34753,\n",
       " 'hindered': 16829,\n",
       " \"martha's\": 30607,\n",
       " 'cheree': 52296,\n",
       " \"pluckin'\": 52297,\n",
       " 'ogles': 40944,\n",
       " 'heavyweight': 11935,\n",
       " 'aada': 82193,\n",
       " 'chopping': 11315,\n",
       " 'strongboy': 61537,\n",
       " 'hegemonic': 41345,\n",
       " 'adorns': 40945,\n",
       " 'xxth': 41349,\n",
       " 'nobuhiro': 34754,\n",
       " 'capites': 52301,\n",
       " 'kavogianni': 52302,\n",
       " 'antwerp': 13425,\n",
       " 'celebrated': 6541,\n",
       " 'roarke': 52303,\n",
       " 'baggins': 40946,\n",
       " 'cheeseburgers': 31273,\n",
       " 'matras': 52304,\n",
       " \"nineties'\": 52305,\n",
       " \"'craig'\": 52306,\n",
       " 'celebrates': 13002,\n",
       " 'unintentionally': 3386,\n",
       " 'drafted': 14365,\n",
       " 'climby': 52307,\n",
       " '303': 52308,\n",
       " 'oldies': 18523,\n",
       " 'climbs': 9099,\n",
       " 'honour': 9658,\n",
       " 'plucking': 34755,\n",
       " '305': 30077,\n",
       " 'address': 5517,\n",
       " 'menjou': 40947,\n",
       " \"'freak'\": 42595,\n",
       " 'dwindling': 19511,\n",
       " 'benson': 9461,\n",
       " 'whites': 52310,\n",
       " 'shamelessness': 40948,\n",
       " 'impacted': 21928,\n",
       " 'upatz': 52311,\n",
       " 'cusack': 3843,\n",
       " \"flavia's\": 37570,\n",
       " 'effette': 52312,\n",
       " 'influx': 34756,\n",
       " 'boooooooo': 52313,\n",
       " 'dimitrova': 52314,\n",
       " 'houseman': 13426,\n",
       " 'bigas': 25262,\n",
       " 'boylen': 52315,\n",
       " 'phillipenes': 52316,\n",
       " 'fakery': 40949,\n",
       " \"grandpa's\": 27661,\n",
       " 'darnell': 27662,\n",
       " 'undergone': 19512,\n",
       " 'handbags': 52318,\n",
       " 'perished': 21929,\n",
       " 'pooped': 37781,\n",
       " 'vigour': 27663,\n",
       " 'opposed': 3630,\n",
       " 'etude': 52319,\n",
       " \"caine's\": 11802,\n",
       " 'doozers': 52320,\n",
       " 'photojournals': 34757,\n",
       " 'perishes': 52321,\n",
       " 'constrains': 34758,\n",
       " 'migenes': 40951,\n",
       " 'consoled': 30608,\n",
       " 'alastair': 16830,\n",
       " 'wvs': 52322,\n",
       " 'ooooooh': 52323,\n",
       " 'approving': 34759,\n",
       " 'consoles': 40952,\n",
       " 'disparagement': 52067,\n",
       " 'futureistic': 52325,\n",
       " 'rebounding': 52326,\n",
       " \"'date\": 52327,\n",
       " 'gregoire': 52328,\n",
       " 'rutherford': 21930,\n",
       " 'americanised': 34760,\n",
       " 'novikov': 82199,\n",
       " 'following': 1045,\n",
       " 'munroe': 34761,\n",
       " \"morita'\": 52329,\n",
       " 'christenssen': 52330,\n",
       " 'oatmeal': 23109,\n",
       " 'fossey': 25263,\n",
       " 'livered': 40953,\n",
       " 'listens': 13003,\n",
       " \"'marci\": 76167,\n",
       " \"otis's\": 52333,\n",
       " 'thanking': 23390,\n",
       " 'maude': 16022,\n",
       " 'extensions': 34762,\n",
       " 'ameteurish': 52335,\n",
       " \"commender's\": 52336,\n",
       " 'agricultural': 27664,\n",
       " 'convincingly': 4521,\n",
       " 'fueled': 17642,\n",
       " 'mahattan': 54017,\n",
       " \"paris's\": 40955,\n",
       " 'vulkan': 52339,\n",
       " 'stapes': 52340,\n",
       " 'odysessy': 52341,\n",
       " 'harmon': 12262,\n",
       " 'surfing': 4255,\n",
       " 'halloran': 23497,\n",
       " 'unbelieveably': 49583,\n",
       " \"'offed'\": 52342,\n",
       " 'quadrant': 30610,\n",
       " 'inhabiting': 19513,\n",
       " 'nebbish': 34763,\n",
       " 'forebears': 40956,\n",
       " 'skirmish': 34764,\n",
       " 'ocassionally': 52343,\n",
       " \"'resist\": 52344,\n",
       " 'impactful': 21931,\n",
       " 'spicier': 52345,\n",
       " 'touristy': 40957,\n",
       " \"'football'\": 52346,\n",
       " 'webpage': 40958,\n",
       " 'exurbia': 52348,\n",
       " 'jucier': 52349,\n",
       " 'professors': 14904,\n",
       " 'structuring': 34765,\n",
       " 'jig': 30611,\n",
       " 'overlord': 40959,\n",
       " 'disconnect': 25264,\n",
       " 'sniffle': 82204,\n",
       " 'slimeball': 40960,\n",
       " 'jia': 40961,\n",
       " 'milked': 16831,\n",
       " 'banjoes': 40962,\n",
       " 'jim': 1240,\n",
       " 'workforces': 52351,\n",
       " 'jip': 52352,\n",
       " 'rotweiller': 52353,\n",
       " 'mundaneness': 34766,\n",
       " \"'ninja'\": 52354,\n",
       " \"dead'\": 11043,\n",
       " \"cipriani's\": 40963,\n",
       " 'modestly': 20611,\n",
       " \"professor'\": 52355,\n",
       " 'shacked': 40964,\n",
       " 'bashful': 34767,\n",
       " 'sorter': 23391,\n",
       " 'overpowering': 16123,\n",
       " 'workmanlike': 18524,\n",
       " 'henpecked': 27665,\n",
       " 'sorted': 18525,\n",
       " \"jb's\": 52357,\n",
       " \"'always\": 52358,\n",
       " \"'baptists\": 34768,\n",
       " 'dreamcatchers': 52359,\n",
       " \"'silence'\": 52360,\n",
       " 'hickory': 21932,\n",
       " 'fun\\x97yet': 52361,\n",
       " 'breakumentary': 52362,\n",
       " 'didn': 15499,\n",
       " 'didi': 52363,\n",
       " 'pealing': 52364,\n",
       " 'dispite': 40965,\n",
       " \"italy's\": 25265,\n",
       " 'instability': 21933,\n",
       " 'quarter': 6542,\n",
       " 'quartet': 12611,\n",
       " 'padm': 52365,\n",
       " \"'bleedmedry\": 52366,\n",
       " 'pahalniuk': 52367,\n",
       " 'honduras': 52368,\n",
       " 'bursting': 10789,\n",
       " \"pablo's\": 41468,\n",
       " 'irremediably': 52370,\n",
       " 'presages': 40966,\n",
       " 'bowlegged': 57835,\n",
       " 'dalip': 65186,\n",
       " 'entering': 6263,\n",
       " 'newsradio': 76175,\n",
       " 'presaged': 54153,\n",
       " \"giallo's\": 27666,\n",
       " 'bouyant': 40967,\n",
       " 'amerterish': 52371,\n",
       " 'rajni': 18526,\n",
       " 'leeves': 30613,\n",
       " 'macauley': 34770,\n",
       " 'seriously': 615,\n",
       " 'sugercoma': 52372,\n",
       " 'grimstead': 52373,\n",
       " \"'fairy'\": 52374,\n",
       " 'zenda': 30614,\n",
       " \"'twins'\": 52375,\n",
       " 'realisation': 17643,\n",
       " 'highsmith': 27667,\n",
       " 'raunchy': 7820,\n",
       " 'incentives': 40968,\n",
       " 'flatson': 52377,\n",
       " 'snooker': 35100,\n",
       " 'crazies': 16832,\n",
       " 'crazier': 14905,\n",
       " 'grandma': 7097,\n",
       " 'napunsaktha': 52378,\n",
       " 'workmanship': 30615,\n",
       " 'reisner': 52379,\n",
       " \"sanford's\": 61309,\n",
       " '\\x91doa': 52380,\n",
       " 'modest': 6111,\n",
       " \"everything's\": 19156,\n",
       " 'hamer': 40969,\n",
       " \"couldn't'\": 52382,\n",
       " 'quibble': 13004,\n",
       " 'socking': 52383,\n",
       " 'tingler': 21934,\n",
       " 'gutman': 52384,\n",
       " 'lachlan': 40970,\n",
       " 'tableaus': 52385,\n",
       " 'headbanger': 52386,\n",
       " 'spoken': 2850,\n",
       " 'cerebrally': 34771,\n",
       " \"'road\": 23493,\n",
       " 'tableaux': 21935,\n",
       " \"proust's\": 40971,\n",
       " 'periodical': 40972,\n",
       " \"shoveller's\": 52388,\n",
       " 'tamara': 25266,\n",
       " 'affords': 17644,\n",
       " 'concert': 3252,\n",
       " \"yara's\": 87958,\n",
       " 'someome': 52389,\n",
       " 'lingering': 8427,\n",
       " \"abraham's\": 41514,\n",
       " 'beesley': 34772,\n",
       " 'cherbourg': 34773,\n",
       " 'kagan': 28627,\n",
       " 'snatch': 9100,\n",
       " \"miyazaki's\": 9263,\n",
       " 'absorbs': 25267,\n",
       " \"koltai's\": 40973,\n",
       " 'tingled': 64030,\n",
       " 'crossroads': 19514,\n",
       " 'rehab': 16124,\n",
       " 'falworth': 52392,\n",
       " 'sequals': 52393,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52256"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['simpsonian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'film',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'casting',\n",
       " 'location',\n",
       " 'scenery',\n",
       " 'story',\n",
       " 'direction',\n",
       " \"everyone's\",\n",
       " 'really',\n",
       " 'suited',\n",
       " 'the',\n",
       " 'part',\n",
       " 'they',\n",
       " 'played',\n",
       " 'and',\n",
       " 'you',\n",
       " 'could',\n",
       " 'just',\n",
       " 'imagine',\n",
       " 'being',\n",
       " 'there',\n",
       " 'robert',\n",
       " \"redford's\",\n",
       " 'is',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'actor',\n",
       " 'and',\n",
       " 'now',\n",
       " 'the',\n",
       " 'same',\n",
       " 'being',\n",
       " 'director',\n",
       " \"norman's\",\n",
       " 'father',\n",
       " 'came',\n",
       " 'from',\n",
       " 'the',\n",
       " 'same',\n",
       " 'scottish',\n",
       " 'island',\n",
       " 'as',\n",
       " 'myself',\n",
       " 'so',\n",
       " 'i',\n",
       " 'loved',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'real',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'this',\n",
       " 'film',\n",
       " 'the',\n",
       " 'witty',\n",
       " 'remarks',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'film',\n",
       " 'were',\n",
       " 'great',\n",
       " 'it',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'so',\n",
       " 'much',\n",
       " 'that',\n",
       " 'i',\n",
       " 'bought',\n",
       " 'the',\n",
       " 'film',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'it',\n",
       " 'was',\n",
       " 'released',\n",
       " 'for',\n",
       " 'retail',\n",
       " 'and',\n",
       " 'would',\n",
       " 'recommend',\n",
       " 'it',\n",
       " 'to',\n",
       " 'everyone',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fly',\n",
       " 'fishing',\n",
       " 'was',\n",
       " 'amazing',\n",
       " 'really',\n",
       " 'cried',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'it',\n",
       " 'was',\n",
       " 'so',\n",
       " 'sad',\n",
       " 'and',\n",
       " 'you',\n",
       " 'know',\n",
       " 'what',\n",
       " 'they',\n",
       " 'say',\n",
       " 'if',\n",
       " 'you',\n",
       " 'cry',\n",
       " 'at',\n",
       " 'a',\n",
       " 'film',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'been',\n",
       " 'good',\n",
       " 'and',\n",
       " 'this',\n",
       " 'definitely',\n",
       " 'was',\n",
       " 'also',\n",
       " 'congratulations',\n",
       " 'to',\n",
       " 'the',\n",
       " 'two',\n",
       " 'little',\n",
       " \"boy's\",\n",
       " 'that',\n",
       " 'played',\n",
       " 'the',\n",
       " \"part's\",\n",
       " 'of',\n",
       " 'norman',\n",
       " 'and',\n",
       " 'paul',\n",
       " 'they',\n",
       " 'were',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'children',\n",
       " 'are',\n",
       " 'often',\n",
       " 'left',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'praising',\n",
       " 'list',\n",
       " 'i',\n",
       " 'think',\n",
       " 'because',\n",
       " 'the',\n",
       " 'stars',\n",
       " 'that',\n",
       " 'play',\n",
       " 'them',\n",
       " 'all',\n",
       " 'grown',\n",
       " 'up',\n",
       " 'are',\n",
       " 'such',\n",
       " 'a',\n",
       " 'big',\n",
       " 'profile',\n",
       " 'for',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'film',\n",
       " 'but',\n",
       " 'these',\n",
       " 'children',\n",
       " 'are',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'should',\n",
       " 'be',\n",
       " 'praised',\n",
       " 'for',\n",
       " 'what',\n",
       " 'they',\n",
       " 'have',\n",
       " 'done',\n",
       " \"don't\",\n",
       " 'you',\n",
       " 'think',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'story',\n",
       " 'was',\n",
       " 'so',\n",
       " 'lovely',\n",
       " 'because',\n",
       " 'it',\n",
       " 'was',\n",
       " 'true',\n",
       " 'and',\n",
       " 'was',\n",
       " \"someone's\",\n",
       " 'life',\n",
       " 'after',\n",
       " 'all',\n",
       " 'that',\n",
       " 'was',\n",
       " 'shared',\n",
       " 'with',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View an input sentence\n",
    "inv_imdb_word_index = {value: key for key, value in test.items()}\n",
    "[inv_imdb_word_index[index] for index in X_train[0] if index > index_from]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and masking sequence data - Coding tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the inputs to the maximum length using maxlen\n",
    "padded_X_train = pad_sequences(X_train, maxlen=300, padding='post', truncating='pre')\n",
    "padded_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Masking layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([25000, 300, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Masking expects to see (batch, sequence, features)\n",
    "# Create a dummy feature dimension using expand_dims\n",
    "\n",
    "padded_X_train = tf.expand_dims(padded_X_train, -1)\n",
    "padded_X_train = tf.cast(padded_X_train, 'float32')\n",
    "padded_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can expand it with `np.expand_dims` then, convert it to tensor with `tf.convert_to_tensor()`\n",
    "\n",
    "```python\n",
    "padded_X_train = np.expand_dims(padded_X_train, -1)\n",
    "tf_X_train = tf.convert_to_tensor(padded_X_train, dtype='float32')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a masking layer\n",
    "masking_layer = Masking(mask_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass padded_X_train to it\n",
    "masked_X_train = masking_layer(padded_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300, 1), dtype=float32, numpy=\n",
       "array([[[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [2.200e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.940e+02],\n",
       "        [1.153e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [4.700e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.100e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.446e+03],\n",
       "        [7.079e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.700e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300, 1), dtype=float32, numpy=\n",
       "array([[[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [2.200e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.940e+02],\n",
       "        [1.153e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [4.700e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.100e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.446e+03],\n",
       "        [7.079e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.700e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-b0f845e22e46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpadded_X_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_mask'"
     ]
    }
   ],
   "source": [
    "padded_X_train._keras_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25000, 300), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False]])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_X_train._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text Data\n",
    "\n",
    "In this section, you will learn how to tokenise text data using `tf.keras.preprocessing.text.Tokenizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text dataset\n",
    "\n",
    "The text we will work with in this notebook is Three Men in a Boat by Jerome K. Jerome, a comical short story about the perils of going outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('./dataset/ThreeMenInABoat.txt', 'r', encoding='utf-8') as f:\n",
    "    text_string = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some simple preprocessing, replacing dashes with empty spaces\n",
    "text_string = text_string.replace('', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER I.   Three invalids.Sufferings of George and Harris.A victim to one hundred and seven fatal maladies.Useful prescriptions.Cure for liver complaint in children.We agree that we are overworked, and need rest.A week on the rolling deep?George suggests the River.Montmorency lodges an objection.Original motion carried by majority of three to one.  There were four of usGeorge, and William Samuel Harris, and myself, and Montmorency.  We were sitting in my room, smoking, and talking about how bad we werebad from a medical point of view I mean, of course.  We were all feeling seedy, and we were getting quite nervous about it. Harris said he felt such extraordinary fits of giddiness come over him at times, that he hardly knew what he was doing; and then George said that _he_ had fits of giddiness too, and hardly knew what _he_ was doing. With me, it was my liver that was out of order.  I knew it was my liver that was out of order, because I had just been reading a patent liver-pill circular, in which were detailed the various symptoms by which a man could tell when his liver was out of order.  I had them all.  It is a most extraordinary thing, but I never read a patent medicine advertisement without being impelled to the conclusion that I am suffering from the particular disease therein dealt with in its most virulent form.  The diagnosis seems in every case to correspond exactly with all the sensations that I have ever felt.  [Picture: Man reading book] I remember going to the British Museum one day to read up the treatment for some slight ailment of which I had a touchhay fever, I fancy it was.  I got down the book, and read all I came to read; and then, in an unthinking moment, I idly turned the leaves, and began to indolently study diseases, generally.  I forget which was the first distemper I plunged intosome fearful, devastating scourge, I knowand, before I had glanced half down the list of premonitory symptoms, it was borne in upon me that I had fairly got it.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_string[:2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  I got down the book, and read all I came to read; and then, in an unthinking moment, I idly turned the leaves, and began to indolently study diseases, generally',\n",
       " '  I forget which was the first distemper I plunged intosome fearful, devastating scourge, I knowand, before I had glanced half down the list of premonitory symptoms, it was borne in upon me that I had fairly got it',\n",
       " '  I sat for awhile, frozen with horror; and then, in the listlessness of despair, I again turned over the pages',\n",
       " '  I came to typhoid feverread the symptomsdiscovered that I had typhoid fever, must have had it for months without knowing itwondered what else I had got; turned up St',\n",
       " ' Vituss Dancefound, as I expected, that I had that too,began to get interested in my case, and determined to sift it to the bottom, and so started alphabeticallyread up ague, and learnt that I was sickening for it, and that the acute stage would commence in about another fortnight',\n",
       " '  Brights disease, I was relieved to find, I had only in a modified form, and, so far as that was concerned, I might live for years',\n",
       " '  Cholera I had, with severe complications; and diphtheria I seemed to have been born with',\n",
       " '  I plodded conscientiously through the twenty-six letters, and the only malady I could conclude I had not got was housemaids knee',\n",
       " '  I felt rather hurt about this at first; it seemed somehow to be a sort of slight',\n",
       " '  Why hadnt I got housemaids knee?  Why this invidious reservation?  After a while, however, less grasping feelings prevailed']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the text into sentences\n",
    "sentence_strings = text_string.split('.')\n",
    "sentence_strings[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Tokenizer object\n",
    "\n",
    "The `Tokenizer` object allows you to easily tokenise words or characters from a text document. It has several options to allow you to adjust the tokenisation process. Documentation is available for the `Tokenizer` [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define any additional characters that we want to filter out (ignore) from the text\n",
    "additional_filters = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer has a `filters` keyword argument, that determines which characters will be filtered out from the text. The cell below shows the default characters that are filtered, to which we are adding our additional filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' + additional_filters,\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token='<UNK>',\n",
    "    document_count=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all, the `Tokenizer` has the following keyword arguments:\n",
    "\n",
    "`num_words`: int. the maximum number of words to keep, based on word frequency. Only the most common `num_words-1` words will be kept. If set to `None`, all words are kept.\n",
    "    \n",
    "`filters`: str. Each element is a character that will be filtered from the texts. Defaults to all punctuation (inc. tabs and line breaks), except `'`.\n",
    "\n",
    "`lower`: bool. Whether to convert the texts to lowercase. Defaults to `True`.\n",
    "\n",
    "`split`: str. Separator for word splitting. Defaults to `' '`.\n",
    "    \n",
    "`char_level`: bool. if True, every character will be treated as a token. Defaults to `False`.\n",
    "\n",
    "`oov_token`: if given, it will be added to word_index and used to replace out-of-vocabulary words during sequence_to_text calls. Defaults to `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Tokenizer to the text\n",
    "\n",
    "We can now tokenize our text using the `fit_on_texts` method. This method takes a list of strings to tokenize, as we have prepared with `sentence_strings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Tokenizer vocabulary\n",
    "tokenizer.fit_on_texts(sentence_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit_on_texts` method could also take a list of lists of strings, and in this case it would recognise each element of each sublist as an individual token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Tokenizer configuration\n",
    "\n",
    "Now that the Tokenizer has ingested the data, we can see what it has extracted from the text by viewing its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['num_words', 'filters', 'lower', 'split', 'char_level', 'oov_token', 'document_count', 'word_counts', 'word_docs', 'index_docs', 'index_word', 'word_index'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tokenizer config as a python dict\n",
    "tokenizer_config = tokenizer.get_config()\n",
    "tokenizer_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"chapter\": 19, \"i\": 1195, \"three\": 79, \"invalids\": 1, \"sufferings\": 2, \"of\": 1487, \"george\": 306, \"and\": 3375, \"harris\": 314, \"a\": 1696, \"victim\": 3, \"to\": 1785, \"one\": 241, \"hundred\": 19, \"seven\": 15, \"fatal\": 1, \"maladies\": 2, \"useful\": 2, \"prescriptions\": 1, \"cure\": 1, \"for\": 525, \"liver\": 8, \"complaint\": 2, \"in\": 976, \"children\": 13, \"we\": 866, \"agree\": 2, \"that\": 944, \"are\": 181, \"overworked\": 1, \"need\": 7, \"rest\": 14, \"week\": 19, \"on\": 501, \"the\": 3603, \"rolling\": 1, \"deep\": 18, \"suggests'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_config['word_counts'][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the number of times each word appears in the corpus. As you can see, the word counts dictionaries in the config are serialized into plain JSON. The `loads()` method in the Python library `json` can be used to convert this JSON string into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "word_counts = json.loads(tokenizer_config['word_counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word index is derived from the `word_counts`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"<UNK>\": 1, \"the\": 2, \"and\": 3, \"to\": 4, \"a\": 5, \"of\": 6, \"it\": 7, \"i\": 8, \"in\": 9, \"that\": 10, \"he\": 11, \"we\": 12, \"was\": 13, \"you\": 14, \"had\": 15, \"for\": 16, \"at\": 17, \"on\": 18, \"with\": 19, \"up\": 20, \"they\": 21, \"is\": 22, \"as\": 23, \"not\": 24, \"his\": 25, \"said\": 26, \"but\": 27, \"would\": 28, \"all\": 29, \"s\": 30, \"have\": 31, \"him\": 32, \"there\": 33, \"be\": 34, \"harris\": 35, \"george\": 36, \"out\": 37, \"t\": 38, \"so\": 39, \"then\": 40, \"when\": 41, \"them\": 42, \"one\": 43, \"were\": 44, \"about\": 45, \"us\": 46, \"'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_config['word_index'][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = json.loads(tokenizer_config['index_word'])\n",
    "word_index = json.loads(tokenizer_config['word_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the sentences to tokens\n",
    "\n",
    "You can map each sentence to a sequence of integer tokens using the Tokenizer's `texts_to_sequences()` method. As was the case for the IMDb data set, the number corresponding to a word is that word's frequency rank in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER I',\n",
       " '   Three invalids',\n",
       " 'Sufferings of George and Harris',\n",
       " 'A victim to one hundred and seven fatal maladies',\n",
       " 'Useful prescriptions']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_strings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "sentence_seq = tokenizer.texts_to_sequences(sentence_strings)\n",
    "type(sentence_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[362, 8],\n",
       " [126, 3362],\n",
       " [2319, 6, 36, 3, 35],\n",
       " [5, 1779, 4, 43, 363, 3, 468, 3363, 2320],\n",
       " [2321, 3364]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362 8\n",
      "126 3362\n",
      "2319 6 36 3 35\n",
      "5 1779 4 43 363 3 468 3363 2320\n",
      "2321 3364\n"
     ]
    }
   ],
   "source": [
    "print(word_index['chapter'], word_index['i'])\n",
    "print(word_index['three'], word_index['invalids'])\n",
    "print(word_index['sufferings'], word_index['of'], word_index['george'], word_index['and'], word_index['harris'])\n",
    "print(word_index['a'], word_index['victim'], word_index['to'], word_index['one'], word_index['hundred'], word_index['and'], word_index['seven'], word_index['fatal'], word_index['maladies'])\n",
    "print(word_index['useful'], word_index['prescriptions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the tokens to sentences\n",
    "\n",
    "You can map the tokens back to sentences using the Tokenizer's `sequences_to_texts` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[362, 8],\n",
       " [126, 3362],\n",
       " [2319, 6, 36, 3, 35],\n",
       " [5, 1779, 4, 43, 363, 3, 468, 3363, 2320],\n",
       " [2321, 3364]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter i',\n",
       " 'three invalids',\n",
       " 'sufferings of george and harris',\n",
       " 'a victim to one hundred and seven fatal maladies',\n",
       " 'useful prescriptions']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the token sequences back to sentences\n",
    "tokenizer.sequences_to_texts(sentence_seq)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter i\n",
      "three invalids\n",
      "sufferings of george and harris\n",
      "a victim to one hundred and seven fatal maladies\n",
      "useful prescriptions\n"
     ]
    }
   ],
   "source": [
    "# Verify the mappings in the config\n",
    "print(index_word['362'], index_word['8'])\n",
    "print(index_word['126'], index_word['3362'])\n",
    "print(index_word['2319'], index_word['6'], index_word['36'], index_word['3'], index_word['35'])\n",
    "print(index_word['5'], index_word['1779'], index_word['4'], index_word['43'], index_word['363'], index_word['3'], index_word['468'], index_word['3363'], index_word['2320'])\n",
    "print(index_word['2321'], index_word['3364'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good day world', 'montmorency bit my finger']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Any valida sequence of tokens can be converted to text\n",
    "tokenizer.sequences_to_texts([[92, 104, 241], [152, 169, 53, 2491]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a word is not featured in the Tokenizer's word index, then it will be mapped to the value of the Tokenizer's `oov_token` property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 28, 78, 1, 1]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize unrecognized words\n",
    "tokenizer.texts_to_sequences(['i would like goobleydoobly hobbledyho'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK>'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the OOV token\n",
    "index_word['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading and resources\n",
    "\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim=1000, output_dim=32, input_length=64, mask_zero=True)\n",
    "test_input = np.random.randint(1000, size=(16, 64))\n",
    "\n",
    "embedded_inputs = embedding_layer(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 64, 32])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (batch_size, sequence, embedding_dim)\n",
    "embedded_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 64, 32), dtype=float32, numpy=\n",
       "array([[[-1.61296949e-02,  1.24746077e-02,  3.67414840e-02, ...,\n",
       "         -3.33563462e-02, -2.48904359e-02,  2.42688097e-02],\n",
       "        [ 3.08068506e-02,  1.22034065e-02, -4.02716994e-02, ...,\n",
       "         -8.57261568e-03, -1.17644295e-02, -1.30997524e-02],\n",
       "        [ 4.81164195e-02, -9.54660028e-03, -4.36462462e-04, ...,\n",
       "         -3.41039300e-02, -1.27578974e-02,  1.05516315e-02],\n",
       "        ...,\n",
       "        [ 3.18861343e-02,  2.79028676e-02,  2.09212303e-05, ...,\n",
       "         -4.15825360e-02, -4.37909029e-02, -4.39045094e-02],\n",
       "        [-1.22934096e-02, -4.91429567e-02,  3.76671441e-02, ...,\n",
       "         -4.05125730e-02, -1.39797106e-02,  6.65520504e-03],\n",
       "        [ 4.45641987e-02, -4.24829870e-03, -9.10187885e-03, ...,\n",
       "          1.64895318e-02,  3.11534740e-02, -8.64217430e-03]],\n",
       "\n",
       "       [[-2.34182011e-02,  5.80786541e-03,  2.23947056e-02, ...,\n",
       "          3.67779471e-02, -2.40232479e-02,  4.77632545e-02],\n",
       "        [ 2.72763409e-02,  1.28341056e-02, -1.35398991e-02, ...,\n",
       "          2.23910548e-02,  3.49131860e-02,  1.92481615e-02],\n",
       "        [ 4.92978953e-02,  1.49632953e-02,  7.49044493e-03, ...,\n",
       "          1.37647875e-02, -1.88389067e-02,  4.48675267e-02],\n",
       "        ...,\n",
       "        [-1.09564066e-02, -1.61117539e-02, -6.33016974e-03, ...,\n",
       "         -3.30996066e-02,  4.90337349e-02, -1.48794055e-02],\n",
       "        [-2.21350435e-02, -2.31275912e-02, -3.84094603e-02, ...,\n",
       "          2.78564952e-02,  2.28947438e-02, -4.50537913e-02],\n",
       "        [-1.89970266e-02,  4.32597883e-02, -3.16326022e-02, ...,\n",
       "          4.49287929e-02,  4.00998853e-02,  4.26833369e-02]],\n",
       "\n",
       "       [[ 3.13145407e-02,  4.77724560e-02,  2.41701044e-02, ...,\n",
       "         -4.22403328e-02, -3.57432850e-02,  2.32270695e-02],\n",
       "        [ 4.98789549e-03,  4.41558249e-02,  2.45223753e-02, ...,\n",
       "         -2.96433810e-02,  2.51989104e-02,  8.42807442e-03],\n",
       "        [ 1.86533369e-02,  7.58962706e-03, -2.93275118e-02, ...,\n",
       "          1.82156675e-02, -3.77413519e-02, -4.98430617e-02],\n",
       "        ...,\n",
       "        [-3.64089981e-02, -2.03331113e-02, -1.58810727e-02, ...,\n",
       "          1.97486952e-03,  3.42172422e-02, -3.32928821e-03],\n",
       "        [ 2.65694894e-02,  2.41514780e-02,  2.34665163e-02, ...,\n",
       "          4.68697585e-02, -4.89161983e-02, -1.63018927e-02],\n",
       "        [ 3.32315080e-02,  2.60208510e-02,  4.25277092e-02, ...,\n",
       "         -3.33186984e-02,  1.51649453e-02, -4.04791459e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.01731566e-02, -6.97641447e-03,  2.39616372e-02, ...,\n",
       "          3.02206911e-02,  2.37751938e-02,  2.79008485e-02],\n",
       "        [-7.93115050e-03, -3.54021080e-02, -4.01880369e-02, ...,\n",
       "          3.77748050e-02, -2.29255091e-02,  4.28431295e-02],\n",
       "        [ 1.05527639e-02, -3.45286839e-02, -4.31136042e-03, ...,\n",
       "         -3.10899261e-02, -1.50924325e-02, -3.07156090e-02],\n",
       "        ...,\n",
       "        [ 4.38794978e-02,  2.67927386e-02,  2.70328633e-02, ...,\n",
       "         -3.20593119e-02,  1.48136280e-02, -4.83068340e-02],\n",
       "        [-3.00376900e-02, -2.25326419e-02, -2.28407625e-02, ...,\n",
       "          1.49709322e-02,  2.98243277e-02,  2.21212953e-03],\n",
       "        [-3.97554152e-02, -2.83407811e-02,  4.77522127e-02, ...,\n",
       "         -4.52860594e-02, -1.46312229e-02,  2.79226564e-02]],\n",
       "\n",
       "       [[-3.16471681e-02,  1.70378797e-02,  1.35117210e-02, ...,\n",
       "          1.35473944e-02, -1.26355514e-02,  3.41254584e-02],\n",
       "        [-2.51718611e-03,  1.58310644e-02, -4.86621633e-02, ...,\n",
       "         -5.33898920e-03, -2.82625444e-02, -3.33095714e-03],\n",
       "        [-1.10529661e-02,  1.04233623e-02, -2.19460484e-02, ...,\n",
       "         -3.49274874e-02, -2.57300269e-02,  1.96474791e-03],\n",
       "        ...,\n",
       "        [-2.14568377e-02,  5.78127056e-03, -3.19450125e-02, ...,\n",
       "          2.67847292e-02, -4.82524559e-03,  5.80656528e-03],\n",
       "        [-1.63511410e-02,  1.41177438e-02,  3.83219235e-02, ...,\n",
       "         -1.65042989e-02, -3.99423838e-02, -2.16270443e-02],\n",
       "        [-4.51854579e-02,  1.69159211e-02, -1.62131190e-02, ...,\n",
       "         -1.83968619e-03, -2.41235141e-02, -4.05201316e-02]],\n",
       "\n",
       "       [[-2.28999257e-02, -6.33726269e-03,  2.02685334e-02, ...,\n",
       "          2.19516195e-02,  3.13979052e-02,  3.72911952e-02],\n",
       "        [ 2.94995792e-02,  2.35951655e-02, -4.43683863e-02, ...,\n",
       "         -2.37560030e-02,  7.92043284e-03,  3.23644541e-02],\n",
       "        [ 2.60813572e-02, -5.87629154e-03,  1.06193200e-02, ...,\n",
       "         -1.49069801e-02, -4.35324684e-02,  1.12175122e-02],\n",
       "        ...,\n",
       "        [ 2.73199715e-02, -5.62232733e-03,  1.60405971e-02, ...,\n",
       "          4.99632470e-02,  5.40951639e-03,  1.73978880e-03],\n",
       "        [-3.96365300e-02, -1.48245580e-02, -2.53622886e-02, ...,\n",
       "          4.95564938e-03,  1.49610527e-02,  4.31224965e-02],\n",
       "        [ 4.89733480e-02, -1.33695491e-02, -1.93822384e-02, ...,\n",
       "         -5.65767288e-03,  1.07600093e-02, -7.38364458e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 64), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_inputs._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and apply an Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer using layers.Embedding\n",
    "# Specify input_dim, output_dim, input_length\n",
    "embedding_layer = Embedding(input_dim=501, output_dim=16, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 1, 16), dtype=float32, numpy=\n",
       "array([[[[ 0.01387929, -0.00772498,  0.00645738, -0.02080063,\n",
       "           0.00244503, -0.04111462, -0.03991162, -0.03426595,\n",
       "           0.00803614,  0.01048518, -0.03316978, -0.04256259,\n",
       "          -0.01436051, -0.03616661, -0.01606041,  0.00918356]],\n",
       "\n",
       "        [[-0.03213673, -0.03387548, -0.01878724, -0.04078745,\n",
       "           0.00487213, -0.0469829 ,  0.03286036, -0.04742943,\n",
       "           0.01055216, -0.01525929,  0.01304796,  0.03340474,\n",
       "           0.01931891,  0.01926393,  0.04178163, -0.04529655]],\n",
       "\n",
       "        [[-0.02840623, -0.0382055 ,  0.00448633, -0.02452889,\n",
       "           0.03987528, -0.01377994,  0.00791148, -0.02342128,\n",
       "           0.00519866, -0.03569081, -0.04662254,  0.01621225,\n",
       "          -0.00208856,  0.03233189,  0.03340435,  0.02009246]],\n",
       "\n",
       "        [[ 0.03481568,  0.00370995, -0.04327318,  0.02297384,\n",
       "           0.03346399,  0.02746177, -0.03883338,  0.04039767,\n",
       "          -0.02437972,  0.02435801, -0.01382912, -0.01763687,\n",
       "           0.01932294, -0.00792655,  0.00262077,  0.04388768]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect an Embedding layer output for a fixed input\n",
    "sequence_of_indices = tf.constant([[[0], [1], [5], [500]]])\n",
    "sequence_of_embeddings = embedding_layer(sequence_of_indices)\n",
    "sequence_of_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.01387929, -0.00772498,  0.00645738, ..., -0.03616661,\n",
       "         -0.01606041,  0.00918356],\n",
       "        [-0.03213673, -0.03387548, -0.01878724, ...,  0.01926393,\n",
       "          0.04178163, -0.04529655],\n",
       "        [-0.03831716,  0.00978211, -0.0312853 , ...,  0.04384097,\n",
       "         -0.00641006, -0.03946279],\n",
       "        ...,\n",
       "        [ 0.03678194, -0.02632709, -0.02338915, ...,  0.04986474,\n",
       "          0.02151053, -0.02940891],\n",
       "        [ 0.03916825, -0.0021978 , -0.04451491, ..., -0.03176067,\n",
       "          0.03198774, -0.00521585],\n",
       "        [ 0.03481568,  0.00370995, -0.04327318, ..., -0.00792655,\n",
       "          0.00262077,  0.04388768]], dtype=float32)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00175937, -0.03421177, -0.04595032,  0.0034956 ,  0.04491306,\n",
       "        0.00841435, -0.04082681,  0.0258669 , -0.02767842,  0.04690797,\n",
       "       -0.00292357, -0.03996376, -0.03777878,  0.01098241, -0.0163951 ,\n",
       "        0.02653707], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding for the 14th index\n",
    "embedding_layer.get_weights()[0][14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a layer that uses the mask_zero kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_embdding_layer = Embedding(input_dim=501, output_dim=16, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 1), dtype=bool, numpy=\n",
       "array([[[False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]]])>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply this layer to the sequence and see the _keras_mask property\n",
    "masked_sequence_of_embeddings = masking_embdding_layer(sequence_of_indices)\n",
    "masked_sequence_of_embeddings._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding Projector - Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the IMDb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = get_and_pad_imdb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word index\n",
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the keys and values of the word index\n",
    "inv_imdb_word_index = {value: key for key, value in imdb_word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'a',\n",
       " 'great',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'david',\n",
       " 'lynch',\n",
       " 'and',\n",
       " 'have',\n",
       " 'everything',\n",
       " 'that',\n",
       " \"he's\",\n",
       " 'made',\n",
       " 'on',\n",
       " 'dvd',\n",
       " 'except',\n",
       " 'for',\n",
       " 'hotel',\n",
       " 'room',\n",
       " 'the',\n",
       " '2',\n",
       " 'hour',\n",
       " 'twin',\n",
       " 'peaks',\n",
       " 'movie',\n",
       " 'so',\n",
       " 'when',\n",
       " 'i',\n",
       " 'found',\n",
       " 'out',\n",
       " 'about',\n",
       " 'this',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'grabbed',\n",
       " 'it',\n",
       " 'and',\n",
       " 'and',\n",
       " 'what',\n",
       " 'is',\n",
       " 'this',\n",
       " \"it's\",\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'drawn',\n",
       " 'black',\n",
       " 'and',\n",
       " 'white',\n",
       " 'cartoons',\n",
       " 'that',\n",
       " 'are',\n",
       " 'loud',\n",
       " 'and',\n",
       " 'foul',\n",
       " 'mouthed',\n",
       " 'and',\n",
       " 'unfunny',\n",
       " 'maybe',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know',\n",
       " \"what's\",\n",
       " 'good',\n",
       " 'but',\n",
       " 'maybe',\n",
       " 'this',\n",
       " 'is',\n",
       " 'just',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'crap',\n",
       " 'that',\n",
       " 'was',\n",
       " 'on',\n",
       " 'the',\n",
       " 'public',\n",
       " 'under',\n",
       " 'the',\n",
       " 'name',\n",
       " 'of',\n",
       " 'david',\n",
       " 'lynch',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'few',\n",
       " 'bucks',\n",
       " 'too',\n",
       " 'let',\n",
       " 'me',\n",
       " 'make',\n",
       " 'it',\n",
       " 'clear',\n",
       " 'that',\n",
       " 'i',\n",
       " \"didn't\",\n",
       " 'care',\n",
       " 'about',\n",
       " 'the',\n",
       " 'foul',\n",
       " 'language',\n",
       " 'part',\n",
       " 'but',\n",
       " 'had',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'the',\n",
       " 'sound',\n",
       " 'because',\n",
       " 'my',\n",
       " 'neighbors',\n",
       " 'might',\n",
       " 'have',\n",
       " 'all',\n",
       " 'in',\n",
       " 'all',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'highly',\n",
       " 'disappointing',\n",
       " 'release',\n",
       " 'and',\n",
       " 'may',\n",
       " 'well',\n",
       " 'have',\n",
       " 'just',\n",
       " 'been',\n",
       " 'left',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box',\n",
       " 'set',\n",
       " 'as',\n",
       " 'a',\n",
       " 'curiosity',\n",
       " 'i',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'you',\n",
       " \"don't\",\n",
       " 'spend',\n",
       " 'your',\n",
       " 'money',\n",
       " 'on',\n",
       " 'this',\n",
       " '2',\n",
       " 'out',\n",
       " 'of',\n",
       " '10']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_imdb_word_index[index] for index in X_train[100] if index > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Embedding layer into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value\n",
    "max_index_value = max(imdb_word_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an embedding dimension\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Dense\n",
    "\n",
    "# Build a model using sequential\n",
    "# 1. Embedding layer\n",
    "# 2. GlobalAveragePooling1D\n",
    "# 3. Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=False),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Functional API refresher.\n",
    "\n",
    "inputs = Input((None, ))\n",
    "embedding_sequence = Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=False)(inputs)\n",
    "average_embedding = GlobalAveragePooling1D()(embedding_sequence)\n",
    "positive_probability = Dense(1, activation='sigmoid')(average_embedding)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=positive_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile, train, and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.6903 - accuracy: 0.5561 - val_loss: 0.6843 - val_accuracy: 0.6844\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.6734 - accuracy: 0.6767 - val_loss: 0.6567 - val_accuracy: 0.7297\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6346 - accuracy: 0.7518 - val_loss: 0.6086 - val_accuracy: 0.7453\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.5855 - accuracy: 0.7816 - val_loss: 0.5638 - val_accuracy: 0.7391\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.5368 - accuracy: 0.8108 - val_loss: 0.5263 - val_accuracy: 0.7609\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow embedding projector\n",
    "\n",
    "The Tensorflow embedding projector can be found [here](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the embedding layer's weights from the trained model\n",
    "weights = model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word Embeddings to tsv files\n",
    "# Two files: \n",
    "#     one contains the embedding labels (meta.tsv),\n",
    "#     one contains the embeddings (vecs.tsv)\n",
    "\n",
    "k = 0\n",
    "\n",
    "for word, token in word_index.items():\n",
    "    if k != 0:\n",
    "        with open('meta.tsv', 'w', encoding='utf-8') as out_m:\n",
    "            out_m.write('\\n')\n",
    "        with open('vecs.tsv', 'w', encoding='utf-8') as out_v:\n",
    "            out_v.write('\\n')\n",
    "    \n",
    "    with open('vecs.tsv', 'w', encoding='utf-8') as out_v:\n",
    "        out_v.write('\\t'.join([str(x) for x in weights[token]]))\n",
    "    with open('meta.tsv', 'w', encoding='utf-8') as out_m:\n",
    "        out_m.write(word)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network layers\n",
    "\n",
    "### Example - Fixed input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 64, 32)            32000     \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 38,533\n",
      "Trainable params: 38,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(1000, 32, input_length=64),\n",
    "    SimpleRNN(64, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Variable input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 64)                6208      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 38,533\n",
      "Trainable params: 38,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(1000, 32),\n",
    "    SimpleRNN(64, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 57,157\n",
      "Trainable params: 57,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(1000, 32),\n",
    "    LSTM(64, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 64)                18816     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 51,141\n",
      "Trainable params: 51,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(1000, 32),\n",
    "    GRU(64, activation='tanh'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and pass an input to a SimpleRNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleRNN layer and test it\n",
    "simplernn_layer = SimpleRNN(units=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 16), dtype=float32, numpy=\n",
       "array([[ 0.9997009 ,  0.9999994 , -0.998944  ,  0.24281038, -0.9989379 ,\n",
       "        -1.        , -1.        , -1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.9999989 , -1.        , -1.        ,\n",
       "        -1.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tf.constant([[[1., 1.], [2., 2.], [56., -100.]]])\n",
    "layer_output = simplernn_layer(sequence)\n",
    "layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = get_and_pad_imdb_dataset(maxlen=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a recurrent neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value\n",
    "max_index_value = max(imdb_word_index.values())\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 162,145\n",
      "Trainable params: 162,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Using sequential, build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=True),\n",
    "    LSTM(units=16),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "536/536 [==============================] - 266s 497ms/step - loss: 0.4422 - accuracy: 0.7960 - val_loss: 0.3285 - val_accuracy: 0.8656\n",
      "Epoch 2/3\n",
      "536/536 [==============================] - 253s 472ms/step - loss: 0.2797 - accuracy: 0.8903 - val_loss: 0.3131 - val_accuracy: 0.8687\n",
      "Epoch 3/3\n",
      "536/536 [==============================] - 252s 471ms/step - loss: 0.1861 - accuracy: 0.9332 - val_loss: 0.3233 - val_accuracy: 0.8531\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=3, batch_size=32, \n",
    "                    validation_data=(X_test, y_test), validation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['please',\n",
       " 'give',\n",
       " 'this',\n",
       " 'one',\n",
       " 'a',\n",
       " 'miss',\n",
       " 'br',\n",
       " 'br',\n",
       " 'and',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'cast',\n",
       " 'rendered',\n",
       " 'terrible',\n",
       " 'performances',\n",
       " 'the',\n",
       " 'show',\n",
       " 'is',\n",
       " 'flat',\n",
       " 'flat',\n",
       " 'flat',\n",
       " 'br',\n",
       " 'br',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'how',\n",
       " 'michael',\n",
       " 'madison',\n",
       " 'could',\n",
       " 'have',\n",
       " 'allowed',\n",
       " 'this',\n",
       " 'one',\n",
       " 'on',\n",
       " 'his',\n",
       " 'plate',\n",
       " 'he',\n",
       " 'almost',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'know',\n",
       " 'this',\n",
       " \"wasn't\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'work',\n",
       " 'out',\n",
       " 'and',\n",
       " 'his',\n",
       " 'performance',\n",
       " 'was',\n",
       " 'quite',\n",
       " 'so',\n",
       " 'all',\n",
       " 'you',\n",
       " 'madison',\n",
       " 'fans',\n",
       " 'give',\n",
       " 'this',\n",
       " 'a',\n",
       " 'miss']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_imdb_word_index = {value: key for key, value in imdb_word_index.items()}\n",
    "[inv_imdb_word_index[index] for index in X_test[0] if index > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11173803]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the model prediction \n",
    "model.predict(X_test[None, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the corresponding label\n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked RNNs and the Bidirectional wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(None, 10))\n",
    "h = Masking(mask_value=0)(inputs)\n",
    "h = LSTM(units=64)(h)\n",
    "outputs = Dense(5, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None, 10)]        0         \n",
      "_________________________________________________________________\n",
      "masking_2 (Masking)          (None, None, 10)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                19200     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 19,525\n",
      "Trainable params: 19,525\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Stacked RNN model\n",
    "\n",
    "If the `return_sequences` argument sets to True, then the layer returns an output for each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, None, 10)]        0         \n",
      "_________________________________________________________________\n",
      "masking_3 (Masking)          (None, None, 10)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 32)          5504      \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 30,661\n",
      "Trainable params: 30,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(None, 10))\n",
    "h = Masking(mask_value=0)(inputs)\n",
    "h = LSTM(units=32, return_sequences=True)(h)\n",
    "h = LSTM(units=64)(h)\n",
    "outputs = Dense(5, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Bidirectional layer\n",
    "\n",
    "We use bidirection model to take account future context as well as past contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, None, 10)]        0         \n",
      "_________________________________________________________________\n",
      "masking_4 (Masking)          (None, None, 10)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 64)          11008     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 44,357\n",
      "Trainable params: 44,357\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "inputs = Input(shape=(None, 10))\n",
    "h = Masking(mask_value=0)(inputs)\n",
    "h = Bidirectional(LSTM(units=32, return_sequences=True))(h)\n",
    "h = LSTM(units=64)(h)\n",
    "outputs = Dense(5, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use it with normal LSTM layer, not set `return_sequences`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, None, 10)]        0         \n",
      "_________________________________________________________________\n",
      "masking_5 (Masking)          (None, None, 10)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 64)          11008     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 77,701\n",
      "Trainable params: 77,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(None, 10))\n",
    "h = Masking(mask_value=0)(inputs)\n",
    "h = Bidirectional(LSTM(units=32, return_sequences=True))(h)\n",
    "h = Bidirectional(LSTM(units=64))(h)\n",
    "outputs = Dense(5, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set the `merge_mode` parameter in Bidirectional layer. (default operation is `'concat'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, None, 10)]        0         \n",
      "_________________________________________________________________\n",
      "masking_6 (Masking)          (None, None, 10)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, None, 32)          11008     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 128)               49664     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 61,317\n",
      "Trainable params: 61,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(None, 10))\n",
    "h = Masking(mask_value=0)(inputs)\n",
    "h = Bidirectional(LSTM(units=32, return_sequences=True), merge_mode='sum')(h)\n",
    "h = Bidirectional(LSTM(units=64))(h)\n",
    "outputs = Dense(5, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = get_and_pad_imdb_dataset()\n",
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build stacked and bidirectional recurrent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value and specify an embedding dimension\n",
    "max_index_value = max(imdb_word_index.values())\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, None, 32)          6272      \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 174,641\n",
      "Trainable params: 174,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Using Sequential model\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=True),\n",
    "    LSTM(units=32, return_sequences=True),\n",
    "    LSTM(units=32, return_sequences=False),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 8)                 1424      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 161,449\n",
      "Trainable params: 161,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Using Sequential API, build a bidirectional RNN with merge_mode='sum'\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=True),\n",
    "    Bidirectional(layer=LSTM(units=8), merge_mode='sum',\n",
    "                  backward_layer=GRU(units=8, go_backwards=True)),\n",
    "    Dense(units=1, activation='sigmoid')   \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, None, 16)          1600      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 8)                 624       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 162,249\n",
      "Trainable params: 162,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a model featuring both stacked recurrent layers and a bidirectional layer\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_index_value+1, output_dim=embedding_dim),\n",
    "    Bidirectional(layer=LSTM(units=8, return_sequences=True), merge_mode='concat'),\n",
    "    GRU(units=8, return_sequences=False),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "#history = model.fit(X_train, y_train, epochs=3, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
