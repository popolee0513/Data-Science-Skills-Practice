import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords 
from nltk.stem import SnowballStemmer,WordNetLemmatizer
stemmer=SnowballStemmer('english')
stop_words = set(stopwords.words('english'))
lemma=WordNetLemmatizer()
def process(x):
    x=re.sub("[^a-zA-Z]+", " " ,x) #過濾掉非字母
    x=[lemma.lemmatize(w) for w in word_tokenize(str(x).lower())] #做詞形還原（lemmatization)
    x = [w for w in x if not w in stop_words] #remove stopwords
    x=' '.join(x)
    return x
