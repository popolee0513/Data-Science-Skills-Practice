{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(directory):\n",
    "    \"\"\"\n",
    "    Given a directory name, return a dictionary mapping the filename of each\n",
    "    `.txt` file inside that directory to the file's contents as a string.\n",
    "    \"\"\"\n",
    "    text =dict()\n",
    "    data= os.listdir(directory)\n",
    "    for i in data:\n",
    "        f = open(os.path.join(directory,i),'r', encoding='UTF-8')\n",
    "        text[i]= f.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    \"\"\"\n",
    "    Given a document (represented as a string), return a list of all of the\n",
    "    words in that document, in order.\n",
    "\n",
    "    Process document by coverting all words to lowercase, and removing any\n",
    "    punctuation or English stopwords.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    document = document.lower()\n",
    "\n",
    "    word = re.sub('['+string.punctuation+']', ' ', document)\n",
    "    word=  word.replace(\"\\n\",\" \").replace(\"  \",\" \")\n",
    "    word = word.split(\" \")\n",
    "    word = [x  for x in word if x not in stop_words and x!=\"\"]\n",
    "    return word \n",
    "\n",
    "def compute_idfs(documents):\n",
    "    \"\"\"\n",
    "    Given a dictionary of `documents` that maps names of documents to a list\n",
    "    of words, return a dictionary that maps words to their IDF values.\n",
    "\n",
    "    Any word that appears in at least one of the documents should be in the\n",
    "    resulting dictionary.\n",
    "    \"\"\"\n",
    "    count =dict()\n",
    "    for key in documents:\n",
    "        for word in set(documents[key]):\n",
    "            if word not in count:\n",
    "                count[word]=1\n",
    "            else:\n",
    "                count[word]+=1\n",
    "            \n",
    "    for key in count:\n",
    "        count[key]=np.log(len(documents)/count[key])\n",
    "    return count\n",
    "\n",
    "\n",
    "def top_files(query, files, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `files` (a dictionary mapping names of\n",
    "    files to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the filenames of the the `n` top\n",
    "    files that match the query, ranked according to tf-idf.\n",
    "    ########################################################\n",
    "    To find the most relevant documents, weâ€™ll use tf-idf to rank documents based \n",
    "    both on term frequency for words in the query as well as inverse document frequency for words in the query.\n",
    "    \"\"\"\n",
    "    ans=[0]*len(files)\n",
    "    for word in query:\n",
    "        for doc in range(len(files)):\n",
    "            if word in files[list(files.keys())[doc]]:\n",
    "                count =files[list(files.keys())[doc]].count(word)\n",
    "                weight = (np.log(count)) * idfs[word]\n",
    "                ans[doc]+=weight\n",
    "    index=np.argsort(-np.array(ans))[:n]\n",
    "    output = [list(files.keys())[x] for x in index]\n",
    "    return output\n",
    "\n",
    "\n",
    "def top_sentences(query, sentences, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `sentences` (a dictionary mapping\n",
    "    sentences to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the `n` top sentences that match\n",
    "    the query, ranked according to idf. If there are ties, preference should\n",
    "    be given to sentences that have a higher query term density.\n",
    "    \"\"\"\n",
    "    ans=[0]*len(sentences)\n",
    "    ans1=[0]*len(sentences)\n",
    "    ans2=[0]*len(sentences)\n",
    "    for word in query:\n",
    "        for doc in range(len(sentences)):\n",
    "            ans2[doc]=doc\n",
    "            if word in sentences[list(sentences.keys())[doc]]:\n",
    "                weight = idfs[word]\n",
    "                ans[doc]+=weight\n",
    "                ans1[doc]+=1\n",
    "            ans1[doc]/= len(sentences[list(sentences.keys())[doc]])\n",
    "    all_ans=list(map(tuple, zip(ans, ans1,ans2)))\n",
    "    all_ans=sorted(all_ans, key=lambda element: (-element[0], -element[1]))\n",
    "    index=[x[2] for x in all_ans[:n]]\n",
    "    output = [list(sentences.keys())[x] for x in index]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    files = load_files(\"corpus\")\n",
    "    file_words = {\n",
    "        filename: tokenize(files[filename])\n",
    "        for filename in files\n",
    "    }\n",
    "    file_idfs = compute_idfs(file_words)\n",
    "\n",
    "    # Prompt user for query\n",
    "    query = set(tokenize(input(\"Query: \")))\n",
    "\n",
    "    # Determine top file matches according to TF-IDF\n",
    "    filenames = top_files(query, file_words, file_idfs, n=1)\n",
    "    # Extract sentences from top files\n",
    "    sentences = dict()\n",
    "    for filename in filenames:\n",
    "        for passage in files[filename].split(\"\\n\"):\n",
    "            for sentence in nltk.sent_tokenize(passage):\n",
    "                tokens = tokenize(sentence)\n",
    "                if tokens:\n",
    "                    sentences[sentence] = tokens\n",
    "    sentence_idfs = compute_idfs(sentences)\n",
    "    matches = top_sentences(query, sentences, sentence_idfs, n=1)\n",
    "    for match in matches:\n",
    "        print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the types of supervised learning?\n",
      "Types of supervised learning algorithms include Active learning , classification and regression.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do neurons connect in a neural network?\n",
      "Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: When was Python 3.0 released?\n",
      "Python 3.0 was released on 3 December 2008.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
