
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer,WordNetLemmatizer
stemmer=SnowballStemmer('english')
lemma=WordNetLemmatizer()
def process(x):
    x=re.sub("[^a-zA-Z]+", " " ,x) #過濾掉非字母
    x=[lemma.lemmatize(w) for w in word_tokenize(str(x).lower())] #做詞形還原（lemmatization)
    x=' '.join(x)
    return x
